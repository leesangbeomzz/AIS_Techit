{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib created a temporary config/cache directory at C:\\Users\\lee_0\\AppData\\Local\\Temp\\matplotlib-bme9v8bi because the default path (C:\\Users\\lee_0\\.matplotlib) is not a writable directory; it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.\n"
     ]
    }
   ],
   "source": [
    "# MNIST Data Set을 이용해서 이미지 학습을 해 보아요!\n",
    "# Tensorflow Keras를 이용해서 구현을 해 보아요!\n",
    "\n",
    "# 필요한 module import \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense\n",
    "from tensorflow.keras.optimizers import SGD, Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Raw Data Laoding\n",
    "df = pd.read_csv(r\"C:\\Users\\lee_0\\Desktop\\코딩\\ML\\12.07\\train.csv\")\n",
    "# display(df)  # 42000 rows × 785 columns\n",
    "# 확인해보니. . . 784개의 pixel정보가 있어요! label이 target(종속변수예요!)\n",
    "# 독립변수(feature)는 당연히 784개예요!\n",
    "\n",
    "# 결측치나 이상치는 없어요! (잘 정제되서 제공된 데이터이기 때문이예요!)\n",
    "# 하지만 현업데이터를 처리할때는 반드시 체크해야 해요!\n",
    "\n",
    "# 정규화하기 전에 일단 이 이미지가 어떤 이미지인지 눈으로 한번 확인하고 넘어가요!\n",
    "\n",
    "img_data = df.drop('label', axis=1, inplace=False).values\n",
    "\n",
    "# 10장의 그림을 확인해 보아요!\n",
    "# 2행 5열로 출력할 꺼예요! => subplot을 이용해서 그러면 되요!\n",
    "fig = plt.figure()  # 큰 도화지를 준비해요.\n",
    "fig_arr = []  # subplot을 저장하는 list\n",
    "\n",
    "for n in range(10):\n",
    "    fig_arr.append(fig.add_subplot(2, 5, n+1))\n",
    "    fig_arr[n].imshow(img_data[n].reshape(28, 28),\n",
    "                      cmap='gray',\n",
    "                      interpolation='nearest')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# feature enginerring 할게 없어요!\n",
    "\n",
    "x_data = df.drop('label', axis=1, inplace=False).values\n",
    "t_data = df['label'].values  # one-hot encoding처리를 해야 해요!\n",
    "                                        # keras기능을 이용해서 one-hot을 자동으로 처리!\n",
    "\n",
    "# 정규화는 당연히 진행해야 해요!\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(x_data)\n",
    "x_data_norm = scaler.transform(x_data)\n",
    "\n",
    "# train데이터와 test데이터로 분리\n",
    "x_data_train_norm, x_data_test_norm, t_data_train, t_data_test = \\\n",
    "train_test_split(x_data_norm,\n",
    "                 t_data,\n",
    "                 test_size=0.3,\n",
    "                 stratify=t_data,\n",
    "                 random_state=1)\n",
    "\n",
    "# 학습용 데이터 : 29,400개\n",
    "# 평가용 데이터 : 12,600개"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 10)                7850      \n",
      "=================================================================\n",
      "Total params: 7,850\n",
      "Trainable params: 7,850\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 데이터가 준비되었으니. . . 모델 만들고 학습해 보아요!\n",
    "\n",
    "keras_model = Sequential()\n",
    "\n",
    "keras_model.add(Flatten(input_shape=(784,)))\n",
    "keras_model.add(Dense(units=10,\n",
    "                      activation='softmax'))\n",
    "\n",
    "keras_model.compile(optimizer=Adam(learning_rate=1e-4),\n",
    "                    loss='sparse_categorical_crossentropy',\n",
    "                    metrics=['acc'])\n",
    "\n",
    "keras_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "236/236 [==============================] - 1s 2ms/step - loss: 2.0717 - acc: 0.3079 - val_loss: 1.7104 - val_acc: 0.6032\n",
      "Epoch 2/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 1.4699 - acc: 0.6992 - val_loss: 1.2683 - val_acc: 0.7543\n",
      "Epoch 3/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 1.1331 - acc: 0.7759 - val_loss: 1.0167 - val_acc: 0.7981\n",
      "Epoch 4/100\n",
      "236/236 [==============================] - 0s 983us/step - loss: 0.9346 - acc: 0.8062 - val_loss: 0.8609 - val_acc: 0.8248\n",
      "Epoch 5/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.8070 - acc: 0.8267 - val_loss: 0.7571 - val_acc: 0.8410\n",
      "Epoch 6/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.7184 - acc: 0.8413 - val_loss: 0.6826 - val_acc: 0.8515\n",
      "Epoch 7/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.6533 - acc: 0.8519 - val_loss: 0.6270 - val_acc: 0.8597\n",
      "Epoch 8/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.6033 - acc: 0.8613 - val_loss: 0.5835 - val_acc: 0.8643\n",
      "Epoch 9/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.5638 - acc: 0.8674 - val_loss: 0.5488 - val_acc: 0.8696\n",
      "Epoch 10/100\n",
      "236/236 [==============================] - 0s 983us/step - loss: 0.5317 - acc: 0.8728 - val_loss: 0.5207 - val_acc: 0.8743\n",
      "Epoch 11/100\n",
      "236/236 [==============================] - 0s 955us/step - loss: 0.5051 - acc: 0.8763 - val_loss: 0.4969 - val_acc: 0.8781\n",
      "Epoch 12/100\n",
      "236/236 [==============================] - 0s 996us/step - loss: 0.4826 - acc: 0.8802 - val_loss: 0.4768 - val_acc: 0.8806\n",
      "Epoch 13/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.4635 - acc: 0.8831 - val_loss: 0.4594 - val_acc: 0.8833\n",
      "Epoch 14/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.4468 - acc: 0.8866 - val_loss: 0.4446 - val_acc: 0.8871\n",
      "Epoch 15/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.4324 - acc: 0.8892 - val_loss: 0.4318 - val_acc: 0.8893\n",
      "Epoch 16/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.4196 - acc: 0.8916 - val_loss: 0.4202 - val_acc: 0.8917\n",
      "Epoch 17/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.4084 - acc: 0.8944 - val_loss: 0.4102 - val_acc: 0.8934\n",
      "Epoch 18/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.3983 - acc: 0.8961 - val_loss: 0.4017 - val_acc: 0.8937\n",
      "Epoch 19/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.3894 - acc: 0.8977 - val_loss: 0.3933 - val_acc: 0.8957\n",
      "Epoch 20/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.3812 - acc: 0.8997 - val_loss: 0.3859 - val_acc: 0.8981\n",
      "Epoch 21/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.3738 - acc: 0.9009 - val_loss: 0.3796 - val_acc: 0.8988\n",
      "Epoch 22/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.3671 - acc: 0.9028 - val_loss: 0.3739 - val_acc: 0.8981\n",
      "Epoch 23/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.3610 - acc: 0.9044 - val_loss: 0.3684 - val_acc: 0.9009\n",
      "Epoch 24/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.3554 - acc: 0.9056 - val_loss: 0.3636 - val_acc: 0.9014\n",
      "Epoch 25/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.3502 - acc: 0.9071 - val_loss: 0.3593 - val_acc: 0.9024\n",
      "Epoch 26/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.3455 - acc: 0.9074 - val_loss: 0.3550 - val_acc: 0.9029\n",
      "Epoch 27/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.3411 - acc: 0.9085 - val_loss: 0.3514 - val_acc: 0.9043\n",
      "Epoch 28/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.3371 - acc: 0.9095 - val_loss: 0.3479 - val_acc: 0.9049\n",
      "Epoch 29/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.3333 - acc: 0.9106 - val_loss: 0.3446 - val_acc: 0.9048\n",
      "Epoch 30/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.3297 - acc: 0.9111 - val_loss: 0.3417 - val_acc: 0.9049\n",
      "Epoch 31/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.3266 - acc: 0.9120 - val_loss: 0.3391 - val_acc: 0.9068\n",
      "Epoch 32/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.3234 - acc: 0.9122 - val_loss: 0.3364 - val_acc: 0.9066\n",
      "Epoch 33/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.3206 - acc: 0.9130 - val_loss: 0.3339 - val_acc: 0.9070\n",
      "Epoch 34/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.3179 - acc: 0.9136 - val_loss: 0.3316 - val_acc: 0.9083\n",
      "Epoch 35/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.3152 - acc: 0.9141 - val_loss: 0.3300 - val_acc: 0.9094\n",
      "Epoch 36/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.3128 - acc: 0.9146 - val_loss: 0.3279 - val_acc: 0.9097\n",
      "Epoch 37/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.3106 - acc: 0.9156 - val_loss: 0.3260 - val_acc: 0.9100\n",
      "Epoch 38/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.3084 - acc: 0.9156 - val_loss: 0.3241 - val_acc: 0.9100\n",
      "Epoch 39/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.3063 - acc: 0.9162 - val_loss: 0.3224 - val_acc: 0.9100\n",
      "Epoch 40/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.3043 - acc: 0.9166 - val_loss: 0.3208 - val_acc: 0.9104\n",
      "Epoch 41/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.3025 - acc: 0.9170 - val_loss: 0.3193 - val_acc: 0.9102\n",
      "Epoch 42/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.3007 - acc: 0.9178 - val_loss: 0.3182 - val_acc: 0.9111\n",
      "Epoch 43/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.2990 - acc: 0.9177 - val_loss: 0.3168 - val_acc: 0.9104\n",
      "Epoch 44/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.2974 - acc: 0.9183 - val_loss: 0.3157 - val_acc: 0.9109\n",
      "Epoch 45/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.2959 - acc: 0.9186 - val_loss: 0.3143 - val_acc: 0.9116\n",
      "Epoch 46/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.2943 - acc: 0.9190 - val_loss: 0.3132 - val_acc: 0.9121\n",
      "Epoch 47/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.2929 - acc: 0.9196 - val_loss: 0.3122 - val_acc: 0.9126\n",
      "Epoch 48/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.2915 - acc: 0.9198 - val_loss: 0.3111 - val_acc: 0.9121\n",
      "Epoch 49/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.2902 - acc: 0.9198 - val_loss: 0.3100 - val_acc: 0.9124\n",
      "Epoch 50/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.2889 - acc: 0.9202 - val_loss: 0.3090 - val_acc: 0.9129\n",
      "Epoch 51/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.2877 - acc: 0.9205 - val_loss: 0.3083 - val_acc: 0.9128\n",
      "Epoch 52/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.2864 - acc: 0.9207 - val_loss: 0.3073 - val_acc: 0.9138\n",
      "Epoch 53/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.2852 - acc: 0.9218 - val_loss: 0.3065 - val_acc: 0.9138\n",
      "Epoch 54/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.2842 - acc: 0.9218 - val_loss: 0.3056 - val_acc: 0.9143\n",
      "Epoch 55/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.2831 - acc: 0.9219 - val_loss: 0.3048 - val_acc: 0.9141\n",
      "Epoch 56/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.2820 - acc: 0.9218 - val_loss: 0.3040 - val_acc: 0.9145\n",
      "Epoch 57/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.2810 - acc: 0.9219 - val_loss: 0.3032 - val_acc: 0.9150\n",
      "Epoch 58/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.2800 - acc: 0.9227 - val_loss: 0.3027 - val_acc: 0.9148\n",
      "Epoch 59/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.2791 - acc: 0.9229 - val_loss: 0.3019 - val_acc: 0.9155\n",
      "Epoch 60/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.2781 - acc: 0.9227 - val_loss: 0.3012 - val_acc: 0.9155\n",
      "Epoch 61/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.2773 - acc: 0.9233 - val_loss: 0.3007 - val_acc: 0.9150\n",
      "Epoch 62/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.2763 - acc: 0.9236 - val_loss: 0.3004 - val_acc: 0.9151\n",
      "Epoch 63/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.2754 - acc: 0.9239 - val_loss: 0.2994 - val_acc: 0.9158\n",
      "Epoch 64/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.2746 - acc: 0.9244 - val_loss: 0.2988 - val_acc: 0.9163\n",
      "Epoch 65/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.2738 - acc: 0.9245 - val_loss: 0.2983 - val_acc: 0.9165\n",
      "Epoch 66/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.2730 - acc: 0.9246 - val_loss: 0.2978 - val_acc: 0.9160\n",
      "Epoch 67/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.2722 - acc: 0.9248 - val_loss: 0.2971 - val_acc: 0.9172\n",
      "Epoch 68/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.2715 - acc: 0.9253 - val_loss: 0.2970 - val_acc: 0.9156\n",
      "Epoch 69/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.2707 - acc: 0.9250 - val_loss: 0.2963 - val_acc: 0.9168\n",
      "Epoch 70/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.2700 - acc: 0.9249 - val_loss: 0.2958 - val_acc: 0.9163\n",
      "Epoch 71/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.2693 - acc: 0.9257 - val_loss: 0.2954 - val_acc: 0.9175\n",
      "Epoch 72/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.2686 - acc: 0.9256 - val_loss: 0.2950 - val_acc: 0.9160\n",
      "Epoch 73/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.2679 - acc: 0.9261 - val_loss: 0.2947 - val_acc: 0.9162\n",
      "Epoch 74/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.2673 - acc: 0.9262 - val_loss: 0.2940 - val_acc: 0.9162\n",
      "Epoch 75/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.2666 - acc: 0.9260 - val_loss: 0.2936 - val_acc: 0.9172\n",
      "Epoch 76/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.2659 - acc: 0.9261 - val_loss: 0.2934 - val_acc: 0.9168\n",
      "Epoch 77/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.2654 - acc: 0.9264 - val_loss: 0.2929 - val_acc: 0.9170\n",
      "Epoch 78/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.2647 - acc: 0.9270 - val_loss: 0.2925 - val_acc: 0.9175\n",
      "Epoch 79/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.2642 - acc: 0.9266 - val_loss: 0.2923 - val_acc: 0.9173\n",
      "Epoch 80/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.2636 - acc: 0.9275 - val_loss: 0.2919 - val_acc: 0.9179\n",
      "Epoch 81/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.2630 - acc: 0.9273 - val_loss: 0.2917 - val_acc: 0.9170\n",
      "Epoch 82/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.2624 - acc: 0.9271 - val_loss: 0.2914 - val_acc: 0.9180\n",
      "Epoch 83/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.2619 - acc: 0.9276 - val_loss: 0.2910 - val_acc: 0.9177\n",
      "Epoch 84/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.2613 - acc: 0.9278 - val_loss: 0.2907 - val_acc: 0.9179\n",
      "Epoch 85/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.2607 - acc: 0.9279 - val_loss: 0.2907 - val_acc: 0.9172\n",
      "Epoch 86/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.2602 - acc: 0.9278 - val_loss: 0.2901 - val_acc: 0.9180\n",
      "Epoch 87/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.2598 - acc: 0.9281 - val_loss: 0.2898 - val_acc: 0.9180\n",
      "Epoch 88/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.2593 - acc: 0.9281 - val_loss: 0.2896 - val_acc: 0.9184\n",
      "Epoch 89/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.2587 - acc: 0.9287 - val_loss: 0.2895 - val_acc: 0.9179\n",
      "Epoch 90/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.2582 - acc: 0.9287 - val_loss: 0.2888 - val_acc: 0.9182\n",
      "Epoch 91/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.2578 - acc: 0.9286 - val_loss: 0.2887 - val_acc: 0.9189\n",
      "Epoch 92/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.2572 - acc: 0.9289 - val_loss: 0.2884 - val_acc: 0.9187\n",
      "Epoch 93/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.2567 - acc: 0.9292 - val_loss: 0.2880 - val_acc: 0.9192\n",
      "Epoch 94/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.2563 - acc: 0.9292 - val_loss: 0.2881 - val_acc: 0.9182\n",
      "Epoch 95/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.2559 - acc: 0.9290 - val_loss: 0.2875 - val_acc: 0.9189\n",
      "Epoch 96/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.2554 - acc: 0.9292 - val_loss: 0.2874 - val_acc: 0.9196\n",
      "Epoch 97/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.2550 - acc: 0.9292 - val_loss: 0.2871 - val_acc: 0.9197\n",
      "Epoch 98/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.2545 - acc: 0.9294 - val_loss: 0.2869 - val_acc: 0.9189\n",
      "Epoch 99/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.2541 - acc: 0.9298 - val_loss: 0.2867 - val_acc: 0.9192\n",
      "Epoch 100/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.2536 - acc: 0.9297 - val_loss: 0.2866 - val_acc: 0.9196\n"
     ]
    }
   ],
   "source": [
    "# 학습을 진행해요!\n",
    "history = keras_model.fit(x_data_train_norm,\n",
    "                          t_data_train,\n",
    "                          epochs=100,\n",
    "                          verbose=1,\n",
    "                          validation_split=0.2,\n",
    "                          batch_size=100)\n",
    "\n",
    "# learning_rate=1e-4\n",
    "# loss: 0.2535 - acc: 0.9298 - val_loss: 0.2862 - val_acc: 0.9194"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "394/394 [==============================] - 0s 579us/step - loss: 0.2911 - acc: 0.9168\n",
      "[0.2911389470100403, 0.9168254137039185]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.97      0.97      1240\n",
      "           1       0.95      0.97      0.96      1405\n",
      "           2       0.92      0.89      0.90      1253\n",
      "           3       0.89      0.88      0.89      1305\n",
      "           4       0.92      0.93      0.93      1222\n",
      "           5       0.88      0.85      0.87      1139\n",
      "           6       0.94      0.96      0.95      1241\n",
      "           7       0.93      0.91      0.92      1320\n",
      "           8       0.90      0.88      0.89      1219\n",
      "           9       0.87      0.90      0.89      1256\n",
      "\n",
      "    accuracy                           0.92     12600\n",
      "   macro avg       0.92      0.92      0.92     12600\n",
      "weighted avg       0.92      0.92      0.92     12600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 학습이 잘 진행되었어요!\n",
    "# 이제 평가를 진행해 보아요!\n",
    "print(keras_model.evaluate(x_data_test_norm, t_data_test))\n",
    "# loss                           , acc\n",
    "# [0.29073357582092285, 0.9186508059501648]\n",
    "\n",
    "# 결과 report(accuracy, recall, precision, f1)값을 구해보아요!\n",
    "# classification_report()를 이용하는데. . . 주의해야 할 점은 one-hot으로 표현하지 않아요!\n",
    "# 1차원으로 표현해요(label값을 이용한 1차원 값으로 사용)\n",
    "\n",
    "# 확률값을 label값으로 변환해야 해요!\n",
    "predict_label = tf.argmax(keras_model.predict(x_data_test_norm), axis=1).numpy()\n",
    "\n",
    "print(classification_report(t_data_test, predict_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 그래프를 그려보아요!\n",
    "loss = history.history['loss']\n",
    "acc = history.history['acc']\n",
    "\n",
    "val_loss = history.history['val_loss']\n",
    "val_acc = history.history['val_acc']\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "ax1 = fig.add_subplot(1, 2, 1)\n",
    "ax2 = fig.add_subplot(1, 2, 2)\n",
    "\n",
    "ax1.plot(loss, color='r')\n",
    "ax1.plot(val_loss, color='b')\n",
    "\n",
    "ax2.plot(acc, color='r')\n",
    "ax2.plot(val_acc, color='b')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 제출파일 형식으로 결과를 Kaggle에 제출해서\n",
    "# 우리가 작성한 모델의 accuracy를 평가 받아 보아요!\n",
    "\n",
    "# 각자 구현해 주세요!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_2 (Flatten)         (None, 784)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10)                7850      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7,850\n",
      "Trainable params: 7,850\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "336/336 [==============================] - 1s 3ms/step - loss: 1.5756 - acc: 0.5143 - val_loss: 1.1546 - val_acc: 0.6687\n",
      "Epoch 2/100\n",
      "336/336 [==============================] - 1s 2ms/step - loss: 1.0170 - acc: 0.6884 - val_loss: 0.9055 - val_acc: 0.7165\n",
      "Epoch 3/100\n",
      "336/336 [==============================] - 1s 2ms/step - loss: 0.8520 - acc: 0.7254 - val_loss: 0.7945 - val_acc: 0.7402\n",
      "Epoch 4/100\n",
      "336/336 [==============================] - 1s 2ms/step - loss: 0.7667 - acc: 0.7496 - val_loss: 0.7285 - val_acc: 0.7606\n",
      "Epoch 5/100\n",
      "336/336 [==============================] - 1s 2ms/step - loss: 0.7115 - acc: 0.7665 - val_loss: 0.6829 - val_acc: 0.7738\n",
      "Epoch 6/100\n",
      "336/336 [==============================] - 1s 2ms/step - loss: 0.6725 - acc: 0.7789 - val_loss: 0.6500 - val_acc: 0.7864\n",
      "Epoch 7/100\n",
      "336/336 [==============================] - 1s 2ms/step - loss: 0.6423 - acc: 0.7889 - val_loss: 0.6234 - val_acc: 0.7937\n",
      "Epoch 8/100\n",
      "336/336 [==============================] - 1s 2ms/step - loss: 0.6188 - acc: 0.7959 - val_loss: 0.6032 - val_acc: 0.7996\n",
      "Epoch 9/100\n",
      "336/336 [==============================] - 1s 2ms/step - loss: 0.5991 - acc: 0.8028 - val_loss: 0.5856 - val_acc: 0.8043\n",
      "Epoch 10/100\n",
      "336/336 [==============================] - 1s 2ms/step - loss: 0.5830 - acc: 0.8073 - val_loss: 0.5711 - val_acc: 0.8077\n",
      "Epoch 11/100\n",
      "336/336 [==============================] - 1s 2ms/step - loss: 0.5693 - acc: 0.8116 - val_loss: 0.5587 - val_acc: 0.8111\n",
      "Epoch 12/100\n",
      "336/336 [==============================] - 1s 2ms/step - loss: 0.5572 - acc: 0.8147 - val_loss: 0.5484 - val_acc: 0.8151\n",
      "Epoch 13/100\n",
      "336/336 [==============================] - 1s 2ms/step - loss: 0.5471 - acc: 0.8174 - val_loss: 0.5390 - val_acc: 0.8186\n",
      "Epoch 14/100\n",
      "336/336 [==============================] - 1s 2ms/step - loss: 0.5378 - acc: 0.8199 - val_loss: 0.5305 - val_acc: 0.8205\n",
      "Epoch 15/100\n",
      "336/336 [==============================] - 1s 2ms/step - loss: 0.5295 - acc: 0.8214 - val_loss: 0.5232 - val_acc: 0.8217\n",
      "Epoch 16/100\n",
      "336/336 [==============================] - 1s 2ms/step - loss: 0.5221 - acc: 0.8243 - val_loss: 0.5172 - val_acc: 0.8235\n",
      "Epoch 17/100\n",
      "336/336 [==============================] - 1s 2ms/step - loss: 0.5157 - acc: 0.8268 - val_loss: 0.5105 - val_acc: 0.8277\n",
      "Epoch 18/100\n",
      "336/336 [==============================] - 1s 2ms/step - loss: 0.5096 - acc: 0.8283 - val_loss: 0.5052 - val_acc: 0.8274\n",
      "Epoch 19/100\n",
      "336/336 [==============================] - 1s 2ms/step - loss: 0.5043 - acc: 0.8300 - val_loss: 0.5002 - val_acc: 0.8305\n",
      "Epoch 20/100\n",
      "336/336 [==============================] - 1s 2ms/step - loss: 0.4993 - acc: 0.8321 - val_loss: 0.4965 - val_acc: 0.8320\n",
      "Epoch 21/100\n",
      "336/336 [==============================] - 1s 2ms/step - loss: 0.4947 - acc: 0.8335 - val_loss: 0.4914 - val_acc: 0.8349\n",
      "Epoch 22/100\n",
      "336/336 [==============================] - 1s 2ms/step - loss: 0.4904 - acc: 0.8351 - val_loss: 0.4881 - val_acc: 0.8350\n",
      "Epoch 23/100\n",
      "336/336 [==============================] - 1s 2ms/step - loss: 0.4862 - acc: 0.8353 - val_loss: 0.4842 - val_acc: 0.8355\n",
      "Epoch 24/100\n",
      "336/336 [==============================] - 1s 2ms/step - loss: 0.4827 - acc: 0.8368 - val_loss: 0.4808 - val_acc: 0.8346\n",
      "Epoch 25/100\n",
      "336/336 [==============================] - 1s 2ms/step - loss: 0.4794 - acc: 0.8374 - val_loss: 0.4778 - val_acc: 0.8377\n",
      "Epoch 26/100\n",
      "336/336 [==============================] - 1s 2ms/step - loss: 0.4761 - acc: 0.8390 - val_loss: 0.4745 - val_acc: 0.8377\n",
      "Epoch 27/100\n",
      "336/336 [==============================] - 1s 2ms/step - loss: 0.4728 - acc: 0.8391 - val_loss: 0.4730 - val_acc: 0.8392\n",
      "Epoch 28/100\n",
      "336/336 [==============================] - 1s 2ms/step - loss: 0.4701 - acc: 0.8394 - val_loss: 0.4708 - val_acc: 0.8373\n",
      "Epoch 29/100\n",
      "336/336 [==============================] - 1s 2ms/step - loss: 0.4673 - acc: 0.8414 - val_loss: 0.4667 - val_acc: 0.8415\n",
      "Epoch 30/100\n",
      "336/336 [==============================] - 0s 1ms/step - loss: 0.4647 - acc: 0.8417 - val_loss: 0.4647 - val_acc: 0.8411\n",
      "Epoch 31/100\n",
      "336/336 [==============================] - 1s 2ms/step - loss: 0.4624 - acc: 0.8423 - val_loss: 0.4631 - val_acc: 0.8423\n",
      "Epoch 32/100\n",
      "336/336 [==============================] - 1s 2ms/step - loss: 0.4601 - acc: 0.8435 - val_loss: 0.4604 - val_acc: 0.8431\n",
      "Epoch 33/100\n",
      "336/336 [==============================] - 1s 2ms/step - loss: 0.4579 - acc: 0.8447 - val_loss: 0.4593 - val_acc: 0.8430\n",
      "Epoch 34/100\n",
      "336/336 [==============================] - 1s 2ms/step - loss: 0.4559 - acc: 0.8442 - val_loss: 0.4578 - val_acc: 0.8450\n",
      "Epoch 35/100\n",
      "336/336 [==============================] - 1s 2ms/step - loss: 0.4539 - acc: 0.8451 - val_loss: 0.4555 - val_acc: 0.8456\n",
      "Epoch 36/100\n",
      "336/336 [==============================] - 1s 2ms/step - loss: 0.4517 - acc: 0.8457 - val_loss: 0.4540 - val_acc: 0.8461\n",
      "Epoch 37/100\n",
      "336/336 [==============================] - 1s 2ms/step - loss: 0.4502 - acc: 0.8462 - val_loss: 0.4521 - val_acc: 0.8467\n",
      "Epoch 38/100\n",
      "336/336 [==============================] - 1s 2ms/step - loss: 0.4482 - acc: 0.8474 - val_loss: 0.4502 - val_acc: 0.8467\n",
      "Epoch 39/100\n",
      "336/336 [==============================] - 1s 2ms/step - loss: 0.4467 - acc: 0.8481 - val_loss: 0.4489 - val_acc: 0.8475\n",
      "Epoch 40/100\n",
      "336/336 [==============================] - 1s 2ms/step - loss: 0.4450 - acc: 0.8472 - val_loss: 0.4473 - val_acc: 0.8485\n",
      "Epoch 41/100\n",
      "336/336 [==============================] - 1s 2ms/step - loss: 0.4438 - acc: 0.8482 - val_loss: 0.4463 - val_acc: 0.8476\n",
      "Epoch 42/100\n",
      "336/336 [==============================] - 1s 2ms/step - loss: 0.4422 - acc: 0.8485 - val_loss: 0.4451 - val_acc: 0.8496\n",
      "Epoch 43/100\n",
      "336/336 [==============================] - 1s 2ms/step - loss: 0.4405 - acc: 0.8492 - val_loss: 0.4450 - val_acc: 0.8471\n",
      "Epoch 44/100\n",
      "336/336 [==============================] - 1s 2ms/step - loss: 0.4393 - acc: 0.8498 - val_loss: 0.4427 - val_acc: 0.8485\n",
      "Epoch 45/100\n",
      "336/336 [==============================] - 1s 2ms/step - loss: 0.4378 - acc: 0.8502 - val_loss: 0.4425 - val_acc: 0.8490\n",
      "Epoch 46/100\n",
      "336/336 [==============================] - 1s 2ms/step - loss: 0.4365 - acc: 0.8504 - val_loss: 0.4407 - val_acc: 0.8499\n",
      "Epoch 47/100\n",
      "336/336 [==============================] - 1s 2ms/step - loss: 0.4352 - acc: 0.8504 - val_loss: 0.4390 - val_acc: 0.8504\n",
      "Epoch 48/100\n",
      "336/336 [==============================] - 1s 2ms/step - loss: 0.4340 - acc: 0.8518 - val_loss: 0.4388 - val_acc: 0.8494\n",
      "Epoch 49/100\n",
      "336/336 [==============================] - 1s 2ms/step - loss: 0.4330 - acc: 0.8514 - val_loss: 0.4386 - val_acc: 0.8498\n",
      "Epoch 50/100\n",
      "336/336 [==============================] - 1s 2ms/step - loss: 0.4316 - acc: 0.8516 - val_loss: 0.4368 - val_acc: 0.8507\n",
      "Epoch 51/100\n",
      "336/336 [==============================] - 1s 2ms/step - loss: 0.4307 - acc: 0.8524 - val_loss: 0.4362 - val_acc: 0.8502\n",
      "Epoch 52/100\n",
      "336/336 [==============================] - 1s 2ms/step - loss: 0.4294 - acc: 0.8530 - val_loss: 0.4356 - val_acc: 0.8506\n",
      "Epoch 53/100\n",
      "336/336 [==============================] - 1s 2ms/step - loss: 0.4287 - acc: 0.8531 - val_loss: 0.4343 - val_acc: 0.8507\n",
      "Epoch 54/100\n",
      "336/336 [==============================] - 1s 2ms/step - loss: 0.4274 - acc: 0.8535 - val_loss: 0.4341 - val_acc: 0.8504\n",
      "Epoch 55/100\n",
      "336/336 [==============================] - 1s 2ms/step - loss: 0.4267 - acc: 0.8534 - val_loss: 0.4333 - val_acc: 0.8515\n",
      "Epoch 56/100\n",
      "336/336 [==============================] - 1s 2ms/step - loss: 0.4255 - acc: 0.8539 - val_loss: 0.4320 - val_acc: 0.8505\n",
      "Epoch 57/100\n",
      "336/336 [==============================] - 1s 2ms/step - loss: 0.4244 - acc: 0.8542 - val_loss: 0.4313 - val_acc: 0.8518\n",
      "Epoch 58/100\n",
      "336/336 [==============================] - 1s 2ms/step - loss: 0.4237 - acc: 0.8553 - val_loss: 0.4302 - val_acc: 0.8514\n",
      "Epoch 59/100\n",
      "336/336 [==============================] - 1s 2ms/step - loss: 0.4226 - acc: 0.8556 - val_loss: 0.4307 - val_acc: 0.8510\n",
      "Epoch 60/100\n",
      "336/336 [==============================] - 1s 2ms/step - loss: 0.4219 - acc: 0.8548 - val_loss: 0.4290 - val_acc: 0.8517\n",
      "Epoch 61/100\n",
      "336/336 [==============================] - 1s 2ms/step - loss: 0.4211 - acc: 0.8559 - val_loss: 0.4292 - val_acc: 0.8504\n",
      "Epoch 62/100\n",
      "336/336 [==============================] - 1s 2ms/step - loss: 0.4201 - acc: 0.8559 - val_loss: 0.4286 - val_acc: 0.8529\n",
      "Epoch 63/100\n",
      "336/336 [==============================] - 1s 2ms/step - loss: 0.4193 - acc: 0.8557 - val_loss: 0.4276 - val_acc: 0.8531\n",
      "Epoch 64/100\n",
      "336/336 [==============================] - 1s 2ms/step - loss: 0.4183 - acc: 0.8568 - val_loss: 0.4267 - val_acc: 0.8526\n",
      "Epoch 65/100\n",
      "336/336 [==============================] - 1s 2ms/step - loss: 0.4177 - acc: 0.8573 - val_loss: 0.4279 - val_acc: 0.8523\n",
      "Epoch 66/100\n",
      "336/336 [==============================] - 1s 2ms/step - loss: 0.4173 - acc: 0.8573 - val_loss: 0.4265 - val_acc: 0.8523\n",
      "Epoch 67/100\n",
      "336/336 [==============================] - 1s 2ms/step - loss: 0.4164 - acc: 0.8571 - val_loss: 0.4250 - val_acc: 0.8527\n",
      "Epoch 68/100\n",
      "336/336 [==============================] - 1s 2ms/step - loss: 0.4155 - acc: 0.8578 - val_loss: 0.4244 - val_acc: 0.8533\n",
      "Epoch 69/100\n",
      "336/336 [==============================] - 1s 2ms/step - loss: 0.4149 - acc: 0.8568 - val_loss: 0.4248 - val_acc: 0.8523\n",
      "Epoch 70/100\n",
      "336/336 [==============================] - 1s 2ms/step - loss: 0.4139 - acc: 0.8578 - val_loss: 0.4243 - val_acc: 0.8520\n",
      "Epoch 71/100\n",
      "336/336 [==============================] - 1s 2ms/step - loss: 0.4134 - acc: 0.8577 - val_loss: 0.4231 - val_acc: 0.8531\n",
      "Epoch 72/100\n",
      "336/336 [==============================] - 1s 2ms/step - loss: 0.4128 - acc: 0.8582 - val_loss: 0.4225 - val_acc: 0.8530\n",
      "Epoch 73/100\n",
      "336/336 [==============================] - 1s 2ms/step - loss: 0.4122 - acc: 0.8580 - val_loss: 0.4221 - val_acc: 0.8527\n",
      "Epoch 74/100\n",
      "336/336 [==============================] - 1s 2ms/step - loss: 0.4114 - acc: 0.8581 - val_loss: 0.4223 - val_acc: 0.8531\n",
      "Epoch 75/100\n",
      "336/336 [==============================] - 1s 2ms/step - loss: 0.4108 - acc: 0.8593 - val_loss: 0.4211 - val_acc: 0.8531\n",
      "Epoch 76/100\n",
      "336/336 [==============================] - 1s 2ms/step - loss: 0.4101 - acc: 0.8585 - val_loss: 0.4214 - val_acc: 0.8539\n",
      "Epoch 77/100\n",
      "336/336 [==============================] - 1s 2ms/step - loss: 0.4096 - acc: 0.8588 - val_loss: 0.4206 - val_acc: 0.8546\n",
      "Epoch 78/100\n",
      "336/336 [==============================] - 1s 2ms/step - loss: 0.4089 - acc: 0.8592 - val_loss: 0.4208 - val_acc: 0.8546\n",
      "Epoch 79/100\n",
      "336/336 [==============================] - 1s 2ms/step - loss: 0.4082 - acc: 0.8597 - val_loss: 0.4208 - val_acc: 0.8532\n",
      "Epoch 80/100\n",
      "336/336 [==============================] - 1s 2ms/step - loss: 0.4078 - acc: 0.8595 - val_loss: 0.4197 - val_acc: 0.8552\n",
      "Epoch 81/100\n",
      "336/336 [==============================] - 1s 2ms/step - loss: 0.4071 - acc: 0.8596 - val_loss: 0.4190 - val_acc: 0.8533\n",
      "Epoch 82/100\n",
      "336/336 [==============================] - 1s 2ms/step - loss: 0.4066 - acc: 0.8602 - val_loss: 0.4195 - val_acc: 0.8535\n",
      "Epoch 83/100\n",
      "336/336 [==============================] - 1s 2ms/step - loss: 0.4062 - acc: 0.8604 - val_loss: 0.4189 - val_acc: 0.8543\n",
      "Epoch 84/100\n",
      "336/336 [==============================] - 1s 2ms/step - loss: 0.4054 - acc: 0.8607 - val_loss: 0.4185 - val_acc: 0.8532\n",
      "Epoch 85/100\n",
      "336/336 [==============================] - 1s 2ms/step - loss: 0.4051 - acc: 0.8607 - val_loss: 0.4179 - val_acc: 0.8537\n",
      "Epoch 86/100\n",
      "336/336 [==============================] - 1s 2ms/step - loss: 0.4046 - acc: 0.8604 - val_loss: 0.4171 - val_acc: 0.8554\n",
      "Epoch 87/100\n",
      "336/336 [==============================] - 1s 2ms/step - loss: 0.4042 - acc: 0.8602 - val_loss: 0.4170 - val_acc: 0.8549\n",
      "Epoch 88/100\n",
      "336/336 [==============================] - 1s 2ms/step - loss: 0.4034 - acc: 0.8613 - val_loss: 0.4170 - val_acc: 0.8555\n",
      "Epoch 89/100\n",
      "336/336 [==============================] - 1s 2ms/step - loss: 0.4030 - acc: 0.8617 - val_loss: 0.4167 - val_acc: 0.8552\n",
      "Epoch 90/100\n",
      "336/336 [==============================] - 1s 2ms/step - loss: 0.4023 - acc: 0.8623 - val_loss: 0.4160 - val_acc: 0.8548\n",
      "Epoch 91/100\n",
      "336/336 [==============================] - 1s 2ms/step - loss: 0.4019 - acc: 0.8620 - val_loss: 0.4161 - val_acc: 0.8556\n",
      "Epoch 92/100\n",
      "336/336 [==============================] - 1s 2ms/step - loss: 0.4015 - acc: 0.8616 - val_loss: 0.4162 - val_acc: 0.8546\n",
      "Epoch 93/100\n",
      "336/336 [==============================] - 1s 2ms/step - loss: 0.4011 - acc: 0.8620 - val_loss: 0.4162 - val_acc: 0.8551\n",
      "Epoch 94/100\n",
      "336/336 [==============================] - 1s 2ms/step - loss: 0.4007 - acc: 0.8616 - val_loss: 0.4146 - val_acc: 0.8556\n",
      "Epoch 95/100\n",
      "336/336 [==============================] - 1s 2ms/step - loss: 0.4000 - acc: 0.8620 - val_loss: 0.4144 - val_acc: 0.8551\n",
      "Epoch 96/100\n",
      "336/336 [==============================] - 1s 2ms/step - loss: 0.3996 - acc: 0.8624 - val_loss: 0.4143 - val_acc: 0.8564\n",
      "Epoch 97/100\n",
      "336/336 [==============================] - 1s 2ms/step - loss: 0.3992 - acc: 0.8619 - val_loss: 0.4153 - val_acc: 0.8554\n",
      "Epoch 98/100\n",
      "336/336 [==============================] - 1s 2ms/step - loss: 0.3987 - acc: 0.8631 - val_loss: 0.4139 - val_acc: 0.8565\n",
      "Epoch 99/100\n",
      "336/336 [==============================] - 1s 2ms/step - loss: 0.3984 - acc: 0.8627 - val_loss: 0.4138 - val_acc: 0.8554\n",
      "Epoch 100/100\n",
      "336/336 [==============================] - 1s 2ms/step - loss: 0.3978 - acc: 0.8627 - val_loss: 0.4135 - val_acc: 0.8562\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.4360 - acc: 0.8536\n",
      "[0.4360387325286865, 0.8536111116409302]\n",
      "563/563 [==============================] - 1s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.83      0.82      1800\n",
      "           1       0.98      0.95      0.96      1800\n",
      "           2       0.75      0.73      0.74      1800\n",
      "           3       0.86      0.87      0.86      1800\n",
      "           4       0.73      0.80      0.76      1800\n",
      "           5       0.95      0.92      0.94      1800\n",
      "           6       0.65      0.60      0.62      1800\n",
      "           7       0.92      0.93      0.92      1800\n",
      "           8       0.95      0.94      0.94      1800\n",
      "           9       0.94      0.95      0.95      1800\n",
      "\n",
      "    accuracy                           0.85     18000\n",
      "   macro avg       0.85      0.85      0.85     18000\n",
      "weighted avg       0.85      0.85      0.85     18000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 필요한 module import \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "\n",
    "# Raw Data Loading\n",
    "df = pd.read_csv(r\"C:\\Users\\lee_0\\Desktop\\코딩\\ML\\12.07\\fashion-mnist_train.csv\")\n",
    "# display(df)  # 60000 rows × 785 columns\n",
    "\n",
    "# 결측치나 이상치는 없어요! (잘 정데되서 제공된 데이터이기 때문이예요!)\n",
    "# 하지만 현업데이터를 처리할때는 반드시 체크해야 해요!\n",
    "\n",
    "# 정규화하기 전에 일단 이 이미지가 어떤 이미지인지 눈으로 한번 확인하고 넘어가요!\n",
    "\n",
    "# img_data = df.drop('label', axis=1, inplace=False).values\n",
    "\n",
    "# 10장의 그림을 확인해 보아요!\n",
    "# 2행 5열로 출력할 꺼예요! => subplot을 이용해서 그리면 되요!\n",
    "# fig = plt.figure()  # 큰 도화지를 준비해요.\n",
    "# fig_arr = []  # subplot을 저장하는 lilst\n",
    "\n",
    "# for n in range(10):\n",
    "#     fig_arr.append(fig.add_subplot(2, 5, n+1))\n",
    "#     fig_arr[n].imshow(img_data[n].reshape(28, 28),\n",
    "#                       cmap='gray',\n",
    "#                       interpolation='nearest')\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# feature enginerring 할게 없어요!\n",
    "\n",
    "x_data = df.drop('label', axis=1, inplace=False).values\n",
    "t_data = df['label'].values  # one-hot encoding처리를 해야 해요!\n",
    "                                       # keras기능을 이용해서 one-hot을 자동으로 처리!\n",
    "\n",
    "# 정규화는 당연히 진행해야 해요!\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(x_data)\n",
    "x_data_norm = scaler.transform(x_data)\n",
    "\n",
    "# train데이터와 test데이터로 분리\n",
    "x_data_train_norm, x_data_test_norm, t_data_train, t_data_test = \\\n",
    "train_test_split(x_data_norm,\n",
    "                 t_data,\n",
    "                 test_size=0.3,\n",
    "                 stratify=t_data,\n",
    "                 random_state=1)\n",
    "\n",
    "# 데이터가 준비되었으니. . . 모델 만들고 학습해 보아요!\n",
    "\n",
    "keras_model = Sequential()\n",
    "keras_model.add(Flatten(input_shape=(784,)))\n",
    "keras_model.add(Dense(units=10,\n",
    "                      activation='softmax'))\n",
    "keras_model.compile(optimizer=Adam(learning_rate=1e-4),\n",
    "                    loss='sparse_categorical_crossentropy',\n",
    "                    metrics=['acc'])\n",
    "keras_model.summary()\n",
    "\n",
    "# 학습을 진행해요!\n",
    "history = keras_model.fit(x_data_train_norm,\n",
    "                          t_data_train,\n",
    "                          epochs=100,\n",
    "                          verbose=1,\n",
    "                          validation_split=0.2,\n",
    "                          batch_size=100)\n",
    "\n",
    "# learning_rate=1e-4\n",
    "# loss: 0.3978 - acc: 0.8627 - val_loss: 0.4135 - val_acc: 0.8562\n",
    "\n",
    "# 학습이 잘 진행되었어요!\n",
    "# 이제 평가를 진행해 보아요!\n",
    "print(keras_model.evaluate(x_data_test_norm, t_data_test))\n",
    "# loss                          , acc\n",
    "# [0.4360387325286865, 0.8536111116409302]\n",
    "\n",
    "# 결과 report(accuracy, recall, precision, f1)값을 구해보아요!\n",
    "# classification_report()를 이용하는데. . . 주의해야 할 점은 one-hot으로 표현하지 않아요!\n",
    "# 1차원으로 표현해요!(label값을 이용한 1차우너 값으로 사용)\n",
    "\n",
    "# 확률값을 label값으로 변환해야 해요!\n",
    "predict_label = tf.argmax(keras_model.predict(x_data_test_norm), axis=1).numpy()\n",
    "print(classification_report(t_data_test, predict_label))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
