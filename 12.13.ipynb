{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib created a temporary config/cache directory at C:\\Users\\lee_0\\AppData\\Local\\Temp\\matplotlib-me_6l3_s because the default path (C:\\Users\\lee_0\\.matplotlib) is not a writable directory; it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.\n"
     ]
    }
   ],
   "source": [
    "# 앙상블을 구현해 보아요!\n",
    "# 앙상블 - Voting을 Iris를 가지고 구현해 보아요!\n",
    "\n",
    "# 필요한 module import\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raw Data Loading\n",
    "iris = load_iris()\n",
    "\n",
    "# DataFrame으로 변환해서 처리하는게 쉽고 편해요!\n",
    "df = pd.DataFrame(iris.data,\n",
    "                  columns=iris.feature_names)\n",
    "# display(df)\n",
    "\n",
    "df.columns = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']\n",
    "df['target'] = iris.target\n",
    "# display(df)\n",
    "\n",
    "# 결측치와 이상치는 없다고 가정하고 진행!\n",
    "# 중복데이터 처리\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "# 이제 x_data와 t_data를 추출하면 될 거 같아요!\n",
    "# x_data는 4개의 feature\n",
    "x_data = df.drop(['target'],\n",
    "                 axis=1,\n",
    "                 inplace=False).values\n",
    "t_data = df['target'].values\n",
    "\n",
    "# 데이터 분리보다 정규화를 먼저 진행하는게 조금 더 편해요!\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(x_data)\n",
    "x_data_norm = scaler.transform(x_data)\n",
    "\n",
    "# 데이터 분리\n",
    "x_data_train_norm, x_data_test_norm, t_data_train, t_data_test = \\\n",
    "train_test_split(x_data_norm,\n",
    "                 t_data,\n",
    "                 test_size=0.3,\n",
    "                 stratify=t_data,\n",
    "                 random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN 모델의 accuracy : 0.9777777777777777\n",
      "SVM 모델의 accuracy : 1.0\n",
      "Decision Tree 모델의 accuracy : 0.9777777777777777\n"
     ]
    }
   ],
   "source": [
    "# 위에서 정제한 데이터를 이용해서 각각의 Model을 만들어요!\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(x_data_train_norm, t_data_train)\n",
    "knn_acc = accuracy_score(t_data_test, knn.predict(x_data_test_norm))\n",
    "print(f'KNN 모델의 accuracy : {knn_acc}')\n",
    "\n",
    "svm = SVC(kernel='linear',\n",
    "          C=5)\n",
    "svm.fit(x_data_train_norm, t_data_train)\n",
    "svm_acc = accuracy_score(t_data_test, svm.predict(x_data_test_norm))\n",
    "print(f'SVM 모델의 accuracy : {svm_acc}')\n",
    "\n",
    "dt = DecisionTreeClassifier()\n",
    "dt.fit(x_data_train_norm, t_data_train)\n",
    "dt_acc = accuracy_score(t_data_test, dt.predict(x_data_test_norm))\n",
    "print(f'Decision Tree 모델의 accuracy : {dt_acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ensemble 모델(hard voting)의 accuracy : 1.0\n"
     ]
    }
   ],
   "source": [
    "# 우리 앙상블 모델을 만들어요!\n",
    "# hard voting classifier(hvc)\n",
    "\n",
    "hvc = VotingClassifier(estimators=[('KNN', knn),\n",
    "                                   ('SVM', svm),\n",
    "                                   ('DT', dt)],\n",
    "                                   voting='hard')\n",
    "hvc.fit(x_data_train_norm, t_data_train)\n",
    "hvc_acc = accuracy_score(t_data_test, hvc.predict(x_data_test_norm))\n",
    "print(f'ensemble 모델(hard voting)의 accuracy : {hvc_acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree 모델의 accuracy : 0.9777777777777777\n",
      "RandomForest 모델의 accuracy : 1.0\n"
     ]
    }
   ],
   "source": [
    "# 앙상블 bagging에 대해서 구현해 보아요!\n",
    "# Decision Tree를 모아서 만든 Random Forest를 구현해 보아요!\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Raw Data Loading\n",
    "iris = load_iris()\n",
    "\n",
    "# DataFrame으로 변환해서 처리하는게 쉽고 편해요!\n",
    "df = pd.DataFrame(iris.data,\n",
    "                  columns=iris.feature_names)\n",
    "# display(df)\n",
    "\n",
    "df.columns = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']\n",
    "df['target'] = iris.target\n",
    "# display(df)\n",
    "\n",
    "# 결측치와 이상치는 없다고 가정하고 진행!\n",
    "# 중복데이터 처리\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "# 이제 x_data와 t_data를 추출하면 될 거 같아요!\n",
    "# x_data는 4개의 feature\n",
    "x_data = df.drop(['target'],\n",
    "                 axis=1,\n",
    "                 inplace=False).values\n",
    "t_data = df['target'].values\n",
    "\n",
    "# 데이터 분리보다 정규화를 먼저 진행하는게 조금 더 편해요!\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(x_data)\n",
    "x_data_norm = scaler.transform(x_data)\n",
    "\n",
    "# 데이터 분리\n",
    "x_data_train_norm, x_data_test_norm, t_data_train, t_data_test = \\\n",
    "train_test_split(x_data_norm,\n",
    "                 t_data,\n",
    "                 test_size=0.3,\n",
    "                 stratify=t_data,\n",
    "                 random_state=0)\n",
    "\n",
    "dt = DecisionTreeClassifier()\n",
    "dt.fit(x_data_train_norm, t_data_train)\n",
    "dt_acc = accuracy_score(t_data_test, dt.predict(x_data_test_norm))\n",
    "print(f'Decision Tree 모델의 accuracy : {dt_acc}')\n",
    "\n",
    "rfc = RandomForestClassifier(n_estimators=50,\n",
    "                             max_depth=3,\n",
    "                             random_state=20)\n",
    "rfc.fit(x_data_train_norm, t_data_train)\n",
    "rfc_acc = accuracy_score(t_data_test, rfc.predict(x_data_test_norm))\n",
    "print(f'RandomForest 모델의 accuracy : {rfc_acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:14:31] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGB 모델의 accuracy : 0.9555555555555556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lee_0\\anaconda3\\envs\\ml\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Raw Data Loading\n",
    "iris = load_iris()\n",
    "\n",
    "# DataFrame으로 변환해서 처리하는게 쉽고 편해요!\n",
    "df = pd.DataFrame(iris.data,\n",
    "                  columns=iris.feature_names)\n",
    "# display(df)\n",
    "\n",
    "df.columns = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']\n",
    "df['target'] = iris.target\n",
    "# display(df)\n",
    "\n",
    "# 결측치와 이상치는 없다고 가정하고 진행!\n",
    "# 중복데이터 처리\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "# 이제 x_data와 t_data를 추출하면 될 거 같아요!\n",
    "# x_data는 4개의 feature\n",
    "x_data = df.drop(['target'],\n",
    "                 axis=1,\n",
    "                 inplace=False).values\n",
    "t_data = df['target'].values\n",
    "\n",
    "# 데이터 분리보다 정규화를 먼저 진행하는게 조금 더 편해요!\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(x_data)\n",
    "x_data_norm = scaler.transform(x_data)\n",
    "\n",
    "# 데이터 분리\n",
    "x_data_train_norm, x_data_test_norm, t_data_train, t_data_test = \\\n",
    "train_test_split(x_data_norm,\n",
    "                 t_data,\n",
    "                 test_size=0.3,\n",
    "                 stratify=t_data,\n",
    "                 random_state=0)\n",
    "\n",
    "# model\n",
    "\n",
    "xgb = XGBClassifier(n_estimators=50,\n",
    "                    max_depth=3,\n",
    "                    random_state=20)\n",
    "xgb.fit(x_data_train_norm, t_data_train)\n",
    "xgb_acc = accuracy_score(t_data_test, xgb.predict(x_data_test_norm))\n",
    "print(f'XGB 모델의 accuracy : {xgb_acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 79ms/step - loss: 0.6931 - acc: 0.7500\n",
      "정확도는 : [0.6931471824645996, 0.75]\n"
     ]
    }
   ],
   "source": [
    "# 최초의 neural network => Perceptron\n",
    "# 아주 간단하게 생각하면 logistic regression과 같아요!\n",
    "# 대신에 activation 함수를 logistic은 sigmoid를 이용해요!\n",
    "# 하지만 Perceptron은 activation함수로 step function을 사용해요!\n",
    "\n",
    "# 이런 Perceptron은 각종 GATE연산을 학습할 수 있으면\n",
    "# 이를 이용해서 AI를 만들 수 있겠다라고 당시에 생각했어요!\n",
    "# 여기서 말하는 GATE연산은 (AND, OR, NOR, XOR, . . .)\n",
    "\n",
    "# 우리도 Perceptron이 GATE연산을 학습할 수 있는지 확인하기 위해\n",
    "# Logistic Regression을 이용해서 GATE연산을 학습해 볼꺼예요!\n",
    "\n",
    "# Tensorflow Keras로 구현해 보아요!\n",
    "# AND, OR, XOR 연산만 해 보아요!\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Training Data Set\n",
    "x_data = np.array([[0, 0],\n",
    "                   [0, 1],\n",
    "                   [1, 0],\n",
    "                   [1, 1]], dtype=np.float32)\n",
    "# AND GATE에 대한 데이터\n",
    "# t_data = np.array([[0], [0], [0], [1]], dtype=np.float32)\n",
    "# OR GATE에 대한 데이터\n",
    "# t_data = np.array([[0], [1], [1], [1]], dtype=np.float32)\n",
    "# XOR GATE에 대한 데이터\n",
    "t_data = np.array([[0], [1], [1], [0]], dtype=np.float32)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(2,)))\n",
    "model.add(Dense(units=1,\n",
    "                activation='sigmoid'))\n",
    "model.compile(optimizer=Adam(learning_rate=1e-2),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "model.fit(x_data,\n",
    "          t_data,\n",
    "          epochs=10000,\n",
    "          verbose=0)\n",
    "\n",
    "print(f'정확도는 : {model.evaluate(x_data, t_data)}')\n",
    "# 정확도는 : [0.00037980571505613625, 1.0] => AND 연산에 대한 학습 결과\n",
    "# 정확도는 : [0.00022676341177430004, 1.0] => OR 연산에 대한 학습 결과\n",
    "# 정확도는 : [0.6931471824645996, 0.5] => XOR 연산에 대한 학습 결과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 48ms/step - loss: 1.3751e-08 - acc: 1.0000\n",
      "정확도는 : [1.375103941825273e-08, 1.0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "# 이번에는 다중 layer perceptron을 이용해서 GATE연산을 학습해 볼꺼예요!\n",
    "\n",
    "# Training Data Set\n",
    "x_data = np.array([[0, 0],\n",
    "                   [0, 1],\n",
    "                   [1, 0],\n",
    "                   [1, 1]], dtype=np.float32)\n",
    "# AND GATE에 대한 데이터\n",
    "# t_data = np.array([[0], [0], [0], [1]], dtype=np.float32)\n",
    "# OR GATE에 대한 데이터\n",
    "# t_data = np.array([[0], [1], [1], [1]], dtype=np.float32)\n",
    "# XOR GATE에 대한 데이터\n",
    "t_data = np.array([[0], [1], [1], [0]], dtype=np.float32)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(2,)))  # Input Layer\n",
    "\n",
    "# 여러개의 Hidden Layer\n",
    "# Hidden Layer는 Dense Layer를 사용해요!\n",
    "model.add(Dense(units=10,\n",
    "                activation='relu'))\n",
    "model.add(Dense(units=6,\n",
    "                activation='relu'))\n",
    "\n",
    "model.add(Dense(units=1,  # Output Layer\n",
    "                activation='sigmoid'))\n",
    "model.compile(optimizer=Adam(learning_rate=1e-2),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "model.fit(x_data,\n",
    "          t_data,\n",
    "          epochs=30000,\n",
    "          verbose=0)\n",
    "\n",
    "print(f'정확도는 : {model.evaluate(x_data, t_data)}')\n",
    "# 정확도는 : [7.163678095167825e-09, 1.0] => AND 연산에 대한 학습 결과\n",
    "# 정확도는 : [6.875959801533327e-09, 1.0] => OR 연산에 대한 학습 결과\n",
    "# 정확도는 : [1.1167191615868433e-08, 1.0] => XOR 연산에 대한 학습 결과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기존에 실습에서 사용했던 MNIST 데이터셋을 두 가지 방식으로 구현해 볼꺼예요!\n",
    "\n",
    "# 1. Logistic Regression을 이용해서 Multinomial Classification\n",
    "# 2. DNN으로 Multinomial Classification 구현해 볼 꺼예요!\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.4198 - acc: 0.8778 - val_loss: 0.3145 - val_acc: 0.9087\n",
      "Epoch 2/100\n",
      "236/236 [==============================] - 0s 852us/step - loss: 0.2925 - acc: 0.9154 - val_loss: 0.2993 - val_acc: 0.9141\n",
      "Epoch 3/100\n",
      "236/236 [==============================] - 0s 753us/step - loss: 0.2738 - acc: 0.9217 - val_loss: 0.2926 - val_acc: 0.9192\n",
      "Epoch 4/100\n",
      "236/236 [==============================] - 0s 779us/step - loss: 0.2613 - acc: 0.9244 - val_loss: 0.3261 - val_acc: 0.9071\n",
      "Epoch 5/100\n",
      "236/236 [==============================] - 0s 773us/step - loss: 0.2533 - acc: 0.9262 - val_loss: 0.2950 - val_acc: 0.9155\n",
      "Epoch 6/100\n",
      "236/236 [==============================] - 0s 797us/step - loss: 0.2463 - acc: 0.9282 - val_loss: 0.3095 - val_acc: 0.9124\n",
      "Epoch 7/100\n",
      "236/236 [==============================] - 0s 774us/step - loss: 0.2385 - acc: 0.9303 - val_loss: 0.3011 - val_acc: 0.9139\n",
      "Epoch 8/100\n",
      "236/236 [==============================] - 0s 686us/step - loss: 0.2379 - acc: 0.9311 - val_loss: 0.3115 - val_acc: 0.9145\n",
      "Epoch 9/100\n",
      "236/236 [==============================] - 0s 775us/step - loss: 0.2331 - acc: 0.9334 - val_loss: 0.3094 - val_acc: 0.9133\n",
      "Epoch 10/100\n",
      "236/236 [==============================] - 0s 748us/step - loss: 0.2334 - acc: 0.9330 - val_loss: 0.3068 - val_acc: 0.9151\n",
      "Epoch 11/100\n",
      "236/236 [==============================] - 0s 698us/step - loss: 0.2276 - acc: 0.9350 - val_loss: 0.3275 - val_acc: 0.9085\n",
      "Epoch 12/100\n",
      "236/236 [==============================] - 0s 737us/step - loss: 0.2240 - acc: 0.9352 - val_loss: 0.3264 - val_acc: 0.9114\n",
      "Epoch 13/100\n",
      "236/236 [==============================] - 0s 723us/step - loss: 0.2209 - acc: 0.9369 - val_loss: 0.3227 - val_acc: 0.9148\n",
      "Epoch 14/100\n",
      "236/236 [==============================] - 0s 698us/step - loss: 0.2217 - acc: 0.9333 - val_loss: 0.3160 - val_acc: 0.9126\n",
      "Epoch 15/100\n",
      "236/236 [==============================] - 0s 727us/step - loss: 0.2149 - acc: 0.9392 - val_loss: 0.3224 - val_acc: 0.9124\n",
      "Epoch 16/100\n",
      "236/236 [==============================] - 0s 743us/step - loss: 0.2198 - acc: 0.9361 - val_loss: 0.3307 - val_acc: 0.9136\n",
      "Epoch 17/100\n",
      "236/236 [==============================] - 0s 816us/step - loss: 0.2111 - acc: 0.9391 - val_loss: 0.3394 - val_acc: 0.9104\n",
      "Epoch 18/100\n",
      "236/236 [==============================] - 0s 805us/step - loss: 0.2112 - acc: 0.9372 - val_loss: 0.3391 - val_acc: 0.9114\n",
      "Epoch 19/100\n",
      "236/236 [==============================] - 0s 726us/step - loss: 0.2064 - acc: 0.9385 - val_loss: 0.3321 - val_acc: 0.9133\n",
      "Epoch 20/100\n",
      "236/236 [==============================] - 0s 776us/step - loss: 0.2072 - acc: 0.9403 - val_loss: 0.3340 - val_acc: 0.9141\n",
      "Epoch 21/100\n",
      "236/236 [==============================] - 0s 710us/step - loss: 0.2065 - acc: 0.9394 - val_loss: 0.3522 - val_acc: 0.9082\n",
      "Epoch 22/100\n",
      "236/236 [==============================] - 0s 758us/step - loss: 0.2058 - acc: 0.9399 - val_loss: 0.3430 - val_acc: 0.9107\n",
      "Epoch 23/100\n",
      "236/236 [==============================] - 0s 708us/step - loss: 0.2033 - acc: 0.9387 - val_loss: 0.3580 - val_acc: 0.9075\n",
      "Epoch 24/100\n",
      "236/236 [==============================] - 0s 727us/step - loss: 0.2037 - acc: 0.9420 - val_loss: 0.3466 - val_acc: 0.9134\n",
      "Epoch 25/100\n",
      "236/236 [==============================] - 0s 740us/step - loss: 0.2036 - acc: 0.9400 - val_loss: 0.3499 - val_acc: 0.9124\n",
      "Epoch 26/100\n",
      "236/236 [==============================] - 0s 756us/step - loss: 0.1998 - acc: 0.9425 - val_loss: 0.3450 - val_acc: 0.9111\n",
      "Epoch 27/100\n",
      "236/236 [==============================] - 0s 760us/step - loss: 0.1977 - acc: 0.9420 - val_loss: 0.3695 - val_acc: 0.9049\n",
      "Epoch 28/100\n",
      "236/236 [==============================] - 0s 821us/step - loss: 0.2001 - acc: 0.9408 - val_loss: 0.3518 - val_acc: 0.9082\n",
      "Epoch 29/100\n",
      "236/236 [==============================] - 0s 889us/step - loss: 0.1981 - acc: 0.9419 - val_loss: 0.3631 - val_acc: 0.9034\n",
      "Epoch 30/100\n",
      "236/236 [==============================] - 0s 876us/step - loss: 0.1926 - acc: 0.9422 - val_loss: 0.3567 - val_acc: 0.9102\n",
      "Epoch 31/100\n",
      "236/236 [==============================] - 0s 802us/step - loss: 0.1933 - acc: 0.9430 - val_loss: 0.3703 - val_acc: 0.9095\n",
      "Epoch 32/100\n",
      "236/236 [==============================] - 0s 746us/step - loss: 0.1961 - acc: 0.9410 - val_loss: 0.3601 - val_acc: 0.9109\n",
      "Epoch 33/100\n",
      "236/236 [==============================] - 0s 704us/step - loss: 0.1942 - acc: 0.9415 - val_loss: 0.3718 - val_acc: 0.9094\n",
      "Epoch 34/100\n",
      "236/236 [==============================] - 0s 671us/step - loss: 0.1894 - acc: 0.9440 - val_loss: 0.3799 - val_acc: 0.9041\n",
      "Epoch 35/100\n",
      "236/236 [==============================] - 0s 715us/step - loss: 0.1886 - acc: 0.9435 - val_loss: 0.3854 - val_acc: 0.9063\n",
      "Epoch 36/100\n",
      "236/236 [==============================] - 0s 711us/step - loss: 0.1871 - acc: 0.9447 - val_loss: 0.3848 - val_acc: 0.9029\n",
      "Epoch 37/100\n",
      "236/236 [==============================] - 0s 784us/step - loss: 0.1914 - acc: 0.9446 - val_loss: 0.3759 - val_acc: 0.9078\n",
      "Epoch 38/100\n",
      "236/236 [==============================] - 0s 813us/step - loss: 0.1874 - acc: 0.9435 - val_loss: 0.3814 - val_acc: 0.9095\n",
      "Epoch 39/100\n",
      "236/236 [==============================] - 0s 766us/step - loss: 0.1852 - acc: 0.9455 - val_loss: 0.4144 - val_acc: 0.8969\n",
      "Epoch 40/100\n",
      "236/236 [==============================] - 0s 787us/step - loss: 0.1893 - acc: 0.9429 - val_loss: 0.3854 - val_acc: 0.9063\n",
      "Epoch 41/100\n",
      "236/236 [==============================] - 0s 716us/step - loss: 0.1906 - acc: 0.9433 - val_loss: 0.3722 - val_acc: 0.9094\n",
      "Epoch 42/100\n",
      "236/236 [==============================] - 0s 848us/step - loss: 0.1848 - acc: 0.9436 - val_loss: 0.3925 - val_acc: 0.9083\n",
      "Epoch 43/100\n",
      "236/236 [==============================] - 0s 752us/step - loss: 0.1800 - acc: 0.9466 - val_loss: 0.3767 - val_acc: 0.9129\n",
      "Epoch 44/100\n",
      "236/236 [==============================] - 0s 789us/step - loss: 0.1833 - acc: 0.9467 - val_loss: 0.3942 - val_acc: 0.9048\n",
      "Epoch 45/100\n",
      "236/236 [==============================] - 0s 787us/step - loss: 0.1870 - acc: 0.9437 - val_loss: 0.3947 - val_acc: 0.9056\n",
      "Epoch 46/100\n",
      "236/236 [==============================] - 0s 769us/step - loss: 0.1821 - acc: 0.9463 - val_loss: 0.3919 - val_acc: 0.9071\n",
      "Epoch 47/100\n",
      "236/236 [==============================] - 0s 753us/step - loss: 0.1840 - acc: 0.9443 - val_loss: 0.3894 - val_acc: 0.9068\n",
      "Epoch 48/100\n",
      "236/236 [==============================] - 0s 712us/step - loss: 0.1824 - acc: 0.9449 - val_loss: 0.3971 - val_acc: 0.9100\n",
      "Epoch 49/100\n",
      "236/236 [==============================] - 0s 704us/step - loss: 0.1800 - acc: 0.9456 - val_loss: 0.4070 - val_acc: 0.9031\n",
      "Epoch 50/100\n",
      "236/236 [==============================] - 0s 769us/step - loss: 0.1811 - acc: 0.9462 - val_loss: 0.4043 - val_acc: 0.9053\n",
      "Epoch 51/100\n",
      "236/236 [==============================] - 0s 708us/step - loss: 0.1833 - acc: 0.9443 - val_loss: 0.3926 - val_acc: 0.9065\n",
      "Epoch 52/100\n",
      "236/236 [==============================] - 0s 708us/step - loss: 0.1793 - acc: 0.9470 - val_loss: 0.4090 - val_acc: 0.9041\n",
      "Epoch 53/100\n",
      "236/236 [==============================] - 0s 800us/step - loss: 0.1771 - acc: 0.9475 - val_loss: 0.4200 - val_acc: 0.9029\n",
      "Epoch 54/100\n",
      "236/236 [==============================] - 0s 823us/step - loss: 0.1798 - acc: 0.9469 - val_loss: 0.4087 - val_acc: 0.9058\n",
      "Epoch 55/100\n",
      "236/236 [==============================] - 0s 841us/step - loss: 0.1781 - acc: 0.9478 - val_loss: 0.4164 - val_acc: 0.9048\n",
      "Epoch 56/100\n",
      "236/236 [==============================] - 0s 790us/step - loss: 0.1779 - acc: 0.9470 - val_loss: 0.4318 - val_acc: 0.9022\n",
      "Epoch 57/100\n",
      "236/236 [==============================] - 0s 761us/step - loss: 0.1737 - acc: 0.9481 - val_loss: 0.4299 - val_acc: 0.9012\n",
      "Epoch 58/100\n",
      "236/236 [==============================] - 0s 708us/step - loss: 0.1731 - acc: 0.9490 - val_loss: 0.4147 - val_acc: 0.9066\n",
      "Epoch 59/100\n",
      "236/236 [==============================] - 0s 729us/step - loss: 0.1760 - acc: 0.9472 - val_loss: 0.4035 - val_acc: 0.9063\n",
      "Epoch 60/100\n",
      "236/236 [==============================] - 0s 673us/step - loss: 0.1718 - acc: 0.9484 - val_loss: 0.4181 - val_acc: 0.9048\n",
      "Epoch 61/100\n",
      "236/236 [==============================] - 0s 708us/step - loss: 0.1718 - acc: 0.9485 - val_loss: 0.4272 - val_acc: 0.9036\n",
      "Epoch 62/100\n",
      "236/236 [==============================] - 0s 663us/step - loss: 0.1726 - acc: 0.9476 - val_loss: 0.4269 - val_acc: 0.9032\n",
      "Epoch 63/100\n",
      "236/236 [==============================] - 0s 691us/step - loss: 0.1722 - acc: 0.9480 - val_loss: 0.4165 - val_acc: 0.9063\n",
      "Epoch 64/100\n",
      "236/236 [==============================] - 0s 685us/step - loss: 0.1754 - acc: 0.9484 - val_loss: 0.4209 - val_acc: 0.9083\n",
      "Epoch 65/100\n",
      "236/236 [==============================] - 0s 678us/step - loss: 0.1728 - acc: 0.9494 - val_loss: 0.4136 - val_acc: 0.9056\n",
      "Epoch 66/100\n",
      "236/236 [==============================] - 0s 662us/step - loss: 0.1718 - acc: 0.9490 - val_loss: 0.4396 - val_acc: 0.8974\n",
      "Epoch 67/100\n",
      "236/236 [==============================] - 0s 704us/step - loss: 0.1730 - acc: 0.9469 - val_loss: 0.4505 - val_acc: 0.9022\n",
      "Epoch 68/100\n",
      "236/236 [==============================] - 0s 697us/step - loss: 0.1754 - acc: 0.9478 - val_loss: 0.4460 - val_acc: 0.8997\n",
      "Epoch 69/100\n",
      "236/236 [==============================] - 0s 625us/step - loss: 0.1709 - acc: 0.9497 - val_loss: 0.4262 - val_acc: 0.9054\n",
      "Epoch 70/100\n",
      "236/236 [==============================] - 0s 712us/step - loss: 0.1714 - acc: 0.9506 - val_loss: 0.4314 - val_acc: 0.9056\n",
      "Epoch 71/100\n",
      "236/236 [==============================] - 0s 678us/step - loss: 0.1733 - acc: 0.9491 - val_loss: 0.4383 - val_acc: 0.9032\n",
      "Epoch 72/100\n",
      "236/236 [==============================] - 0s 739us/step - loss: 0.1718 - acc: 0.9480 - val_loss: 0.4404 - val_acc: 0.9014\n",
      "Epoch 73/100\n",
      "236/236 [==============================] - 0s 917us/step - loss: 0.1701 - acc: 0.9483 - val_loss: 0.4449 - val_acc: 0.9027\n",
      "Epoch 74/100\n",
      "236/236 [==============================] - 0s 806us/step - loss: 0.1705 - acc: 0.9485 - val_loss: 0.4501 - val_acc: 0.9009\n",
      "Epoch 75/100\n",
      "236/236 [==============================] - 0s 649us/step - loss: 0.1690 - acc: 0.9500 - val_loss: 0.4572 - val_acc: 0.9019\n",
      "Epoch 76/100\n",
      "236/236 [==============================] - 0s 744us/step - loss: 0.1667 - acc: 0.9502 - val_loss: 0.4488 - val_acc: 0.9017\n",
      "Epoch 77/100\n",
      "236/236 [==============================] - 0s 678us/step - loss: 0.1688 - acc: 0.9507 - val_loss: 0.4331 - val_acc: 0.9044\n",
      "Epoch 78/100\n",
      "236/236 [==============================] - 0s 653us/step - loss: 0.1658 - acc: 0.9498 - val_loss: 0.4618 - val_acc: 0.8986\n",
      "Epoch 79/100\n",
      "236/236 [==============================] - 0s 745us/step - loss: 0.1711 - acc: 0.9491 - val_loss: 0.4517 - val_acc: 0.8988\n",
      "Epoch 80/100\n",
      "236/236 [==============================] - 0s 689us/step - loss: 0.1650 - acc: 0.9520 - val_loss: 0.4490 - val_acc: 0.9027\n",
      "Epoch 81/100\n",
      "236/236 [==============================] - 0s 698us/step - loss: 0.1660 - acc: 0.9497 - val_loss: 0.4423 - val_acc: 0.9046\n",
      "Epoch 82/100\n",
      "236/236 [==============================] - 0s 693us/step - loss: 0.1673 - acc: 0.9496 - val_loss: 0.4428 - val_acc: 0.9053\n",
      "Epoch 83/100\n",
      "236/236 [==============================] - 0s 694us/step - loss: 0.1658 - acc: 0.9526 - val_loss: 0.4649 - val_acc: 0.8998\n",
      "Epoch 84/100\n",
      "236/236 [==============================] - 0s 671us/step - loss: 0.1642 - acc: 0.9512 - val_loss: 0.4534 - val_acc: 0.9048\n",
      "Epoch 85/100\n",
      "236/236 [==============================] - 0s 686us/step - loss: 0.1640 - acc: 0.9503 - val_loss: 0.4485 - val_acc: 0.9053\n",
      "Epoch 86/100\n",
      "236/236 [==============================] - 0s 662us/step - loss: 0.1651 - acc: 0.9511 - val_loss: 0.4716 - val_acc: 0.9017\n",
      "Epoch 87/100\n",
      "236/236 [==============================] - 0s 643us/step - loss: 0.1661 - acc: 0.9486 - val_loss: 0.4663 - val_acc: 0.9034\n",
      "Epoch 88/100\n",
      "236/236 [==============================] - 0s 644us/step - loss: 0.1675 - acc: 0.9500 - val_loss: 0.4684 - val_acc: 0.9022\n",
      "Epoch 89/100\n",
      "236/236 [==============================] - 0s 700us/step - loss: 0.1627 - acc: 0.9511 - val_loss: 0.4681 - val_acc: 0.9022\n",
      "Epoch 90/100\n",
      "236/236 [==============================] - 0s 660us/step - loss: 0.1659 - acc: 0.9488 - val_loss: 0.4890 - val_acc: 0.8978\n",
      "Epoch 91/100\n",
      "236/236 [==============================] - 0s 651us/step - loss: 0.1645 - acc: 0.9506 - val_loss: 0.4590 - val_acc: 0.9041\n",
      "Epoch 92/100\n",
      "236/236 [==============================] - 0s 656us/step - loss: 0.1626 - acc: 0.9508 - val_loss: 0.4719 - val_acc: 0.9014\n",
      "Epoch 93/100\n",
      "236/236 [==============================] - 0s 638us/step - loss: 0.1619 - acc: 0.9524 - val_loss: 0.4830 - val_acc: 0.8968\n",
      "Epoch 94/100\n",
      "236/236 [==============================] - 0s 628us/step - loss: 0.1663 - acc: 0.9499 - val_loss: 0.4693 - val_acc: 0.9049\n",
      "Epoch 95/100\n",
      "236/236 [==============================] - 0s 684us/step - loss: 0.1623 - acc: 0.9514 - val_loss: 0.4647 - val_acc: 0.9034\n",
      "Epoch 96/100\n",
      "236/236 [==============================] - 0s 639us/step - loss: 0.1624 - acc: 0.9510 - val_loss: 0.4813 - val_acc: 0.9010\n",
      "Epoch 97/100\n",
      "236/236 [==============================] - 0s 656us/step - loss: 0.1644 - acc: 0.9511 - val_loss: 0.5019 - val_acc: 0.8946\n",
      "Epoch 98/100\n",
      "236/236 [==============================] - 0s 676us/step - loss: 0.1637 - acc: 0.9512 - val_loss: 0.4758 - val_acc: 0.9032\n",
      "Epoch 99/100\n",
      "236/236 [==============================] - 0s 813us/step - loss: 0.1626 - acc: 0.9513 - val_loss: 0.4767 - val_acc: 0.9014\n",
      "Epoch 100/100\n",
      "236/236 [==============================] - 0s 847us/step - loss: 0.1591 - acc: 0.9509 - val_loss: 0.4748 - val_acc: 0.9010\n",
      "394/394 [==============================] - 0s 370us/step - loss: 0.5192 - acc: 0.9016\n",
      "[0.5192373991012573, 0.9015873074531555]\n"
     ]
    }
   ],
   "source": [
    "# Raw Data Loading\n",
    "df = pd.read_csv(r\"C:\\Users\\lee_0\\Desktop\\코딩\\ML\\12.07\\train.csv\")\n",
    "# display(df)  # 42000 rows × 785 columns\n",
    "\n",
    "# 결측치와 이상치가 없어요!\n",
    "# Feature Enginerring을 할게 없어요!\n",
    "\n",
    "# 독립변수(feature), 종속변수(target) 분리\n",
    "x_data = df.drop('label', axis=1, inplace=False).values\n",
    "t_data = df['label'].values  # 원래 one-hot 처리를 해야 해요!\n",
    "                                        # 하지만 keras에게 one-hot 처리를 위임할 수 있어서\n",
    "                                        # 따로 처리는 안할꺼예요!\n",
    "\n",
    "# 정규화는 진행해야 해요!\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(x_data)\n",
    "x_data_norm = scaler.transform(x_data)\n",
    "\n",
    "# train, test 분리\n",
    "x_data_train_norm, x_data_test_norm, t_data_train, t_data_test = \\\n",
    "train_test_split(x_data_norm,\n",
    "                 t_data,\n",
    "                 test_size=0.3,\n",
    "                 stratify=t_data,\n",
    "                 random_state=0)\n",
    "\n",
    "# Model 구현(Regression Model 구현)\n",
    "model = Sequential()\n",
    "\n",
    "# Input Layer\n",
    "model.add(Flatten(input_shape=(784,)))\n",
    "\n",
    "# Output Layer\n",
    "model.add(Dense(units=10,\n",
    "                activation='softmax'))\n",
    "model.compile(optimizer=Adam(learning_rate=1e-2),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['acc'])\n",
    "\n",
    "# Model 학습\n",
    "history = model.fit(x_data_train_norm,\n",
    "                    t_data_train,\n",
    "                    epochs=100,\n",
    "                    batch_size=100,\n",
    "                    validation_split=0.2,\n",
    "                    verbose=1)\n",
    "\n",
    "# Model 평가\n",
    "print(model.evaluate(x_data_test_norm,\n",
    "                     t_data_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.92      0.94      1240\n",
      "           1       0.95      0.97      0.96      1405\n",
      "           2       0.90      0.86      0.88      1253\n",
      "           3       0.85      0.89      0.87      1305\n",
      "           4       0.92      0.91      0.91      1222\n",
      "           5       0.85      0.84      0.85      1139\n",
      "           6       0.91      0.95      0.93      1241\n",
      "           7       0.91      0.92      0.92      1320\n",
      "           8       0.87      0.86      0.86      1219\n",
      "           9       0.87      0.87      0.87      1256\n",
      "\n",
      "    accuracy                           0.90     12600\n",
      "   macro avg       0.90      0.90      0.90     12600\n",
      "weighted avg       0.90      0.90      0.90     12600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "predict = tf.argmax(model.predict(x_data_test_norm), axis=1).numpy()\n",
    "print(classification_report(t_data_test,\n",
    "                            predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "236/236 [==============================] - 1s 2ms/step - loss: 0.3097 - acc: 0.9051 - val_loss: 0.1842 - val_acc: 0.9439\n",
      "Epoch 2/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.1495 - acc: 0.9551 - val_loss: 0.1279 - val_acc: 0.9616\n",
      "Epoch 3/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.1200 - acc: 0.9647 - val_loss: 0.1377 - val_acc: 0.9568\n",
      "Epoch 4/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.1071 - acc: 0.9664 - val_loss: 0.1773 - val_acc: 0.9507\n",
      "Epoch 5/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0918 - acc: 0.9719 - val_loss: 0.1900 - val_acc: 0.9539\n",
      "Epoch 6/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0820 - acc: 0.9754 - val_loss: 0.1687 - val_acc: 0.9592\n",
      "Epoch 7/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0711 - acc: 0.9787 - val_loss: 0.2034 - val_acc: 0.9546\n",
      "Epoch 8/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0782 - acc: 0.9788 - val_loss: 0.2297 - val_acc: 0.9520\n",
      "Epoch 9/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0754 - acc: 0.9781 - val_loss: 0.2571 - val_acc: 0.9544\n",
      "Epoch 10/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0797 - acc: 0.9773 - val_loss: 0.2006 - val_acc: 0.9660\n",
      "Epoch 11/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0604 - acc: 0.9834 - val_loss: 0.1913 - val_acc: 0.9634\n",
      "Epoch 12/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0765 - acc: 0.9791 - val_loss: 0.2528 - val_acc: 0.9524\n",
      "Epoch 13/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0631 - acc: 0.9829 - val_loss: 0.2013 - val_acc: 0.9617\n",
      "Epoch 14/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0565 - acc: 0.9853 - val_loss: 0.2045 - val_acc: 0.9573\n",
      "Epoch 15/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0499 - acc: 0.9869 - val_loss: 0.1965 - val_acc: 0.9662\n",
      "Epoch 16/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0496 - acc: 0.9876 - val_loss: 0.2800 - val_acc: 0.9556\n",
      "Epoch 17/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0763 - acc: 0.9830 - val_loss: 0.2346 - val_acc: 0.9660\n",
      "Epoch 18/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0661 - acc: 0.9845 - val_loss: 0.2554 - val_acc: 0.9588\n",
      "Epoch 19/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0440 - acc: 0.9895 - val_loss: 0.2270 - val_acc: 0.9677\n",
      "Epoch 20/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0370 - acc: 0.9906 - val_loss: 0.2562 - val_acc: 0.9645\n",
      "Epoch 21/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0542 - acc: 0.9879 - val_loss: 0.2483 - val_acc: 0.9622\n",
      "Epoch 22/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0473 - acc: 0.9896 - val_loss: 0.2347 - val_acc: 0.9675\n",
      "Epoch 23/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0401 - acc: 0.9904 - val_loss: 0.2607 - val_acc: 0.9675\n",
      "Epoch 24/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0493 - acc: 0.9892 - val_loss: 0.2884 - val_acc: 0.9684\n",
      "Epoch 25/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0749 - acc: 0.9851 - val_loss: 0.2431 - val_acc: 0.9668\n",
      "Epoch 26/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0482 - acc: 0.9900 - val_loss: 0.2583 - val_acc: 0.9696\n",
      "Epoch 27/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0436 - acc: 0.9911 - val_loss: 0.2582 - val_acc: 0.9653\n",
      "Epoch 28/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0375 - acc: 0.9921 - val_loss: 0.2658 - val_acc: 0.9690\n",
      "Epoch 29/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0274 - acc: 0.9941 - val_loss: 0.2888 - val_acc: 0.9670\n",
      "Epoch 30/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0267 - acc: 0.9937 - val_loss: 0.3725 - val_acc: 0.9604\n",
      "Epoch 31/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0505 - acc: 0.9892 - val_loss: 0.2537 - val_acc: 0.9663\n",
      "Epoch 32/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0481 - acc: 0.9902 - val_loss: 0.2568 - val_acc: 0.9662\n",
      "Epoch 33/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0406 - acc: 0.9918 - val_loss: 0.3263 - val_acc: 0.9670\n",
      "Epoch 34/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0515 - acc: 0.9898 - val_loss: 0.2826 - val_acc: 0.9650\n",
      "Epoch 35/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0317 - acc: 0.9926 - val_loss: 0.3277 - val_acc: 0.9626\n",
      "Epoch 36/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0460 - acc: 0.9927 - val_loss: 0.4676 - val_acc: 0.9597\n",
      "Epoch 37/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0448 - acc: 0.9908 - val_loss: 0.3035 - val_acc: 0.9665\n",
      "Epoch 38/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0290 - acc: 0.9939 - val_loss: 0.2674 - val_acc: 0.9704\n",
      "Epoch 39/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0240 - acc: 0.9948 - val_loss: 0.2998 - val_acc: 0.9709\n",
      "Epoch 40/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0266 - acc: 0.9944 - val_loss: 0.3739 - val_acc: 0.9663\n",
      "Epoch 41/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0459 - acc: 0.9919 - val_loss: 0.2792 - val_acc: 0.9679\n",
      "Epoch 42/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0307 - acc: 0.9935 - val_loss: 0.3350 - val_acc: 0.9638\n",
      "Epoch 43/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0384 - acc: 0.9928 - val_loss: 0.3660 - val_acc: 0.9645\n",
      "Epoch 44/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0536 - acc: 0.9905 - val_loss: 0.3864 - val_acc: 0.9656\n",
      "Epoch 45/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0590 - acc: 0.9895 - val_loss: 0.3283 - val_acc: 0.9651\n",
      "Epoch 46/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0322 - acc: 0.9940 - val_loss: 0.3860 - val_acc: 0.9670\n",
      "Epoch 47/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0280 - acc: 0.9946 - val_loss: 0.4055 - val_acc: 0.9646\n",
      "Epoch 48/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0307 - acc: 0.9954 - val_loss: 0.3753 - val_acc: 0.9713\n",
      "Epoch 49/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0171 - acc: 0.9964 - val_loss: 0.3442 - val_acc: 0.9730\n",
      "Epoch 50/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0133 - acc: 0.9969 - val_loss: 0.3894 - val_acc: 0.9701\n",
      "Epoch 51/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0327 - acc: 0.9932 - val_loss: 0.3918 - val_acc: 0.9665\n",
      "Epoch 52/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.1229 - acc: 0.9832 - val_loss: 0.4124 - val_acc: 0.9662\n",
      "Epoch 53/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0812 - acc: 0.9868 - val_loss: 0.3898 - val_acc: 0.9614\n",
      "Epoch 54/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0409 - acc: 0.9923 - val_loss: 0.3490 - val_acc: 0.9690\n",
      "Epoch 55/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0316 - acc: 0.9935 - val_loss: 0.3501 - val_acc: 0.9668\n",
      "Epoch 56/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0212 - acc: 0.9963 - val_loss: 0.3926 - val_acc: 0.9687\n",
      "Epoch 57/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0236 - acc: 0.9960 - val_loss: 0.3395 - val_acc: 0.9723\n",
      "Epoch 58/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0113 - acc: 0.9974 - val_loss: 0.4107 - val_acc: 0.9702\n",
      "Epoch 59/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0130 - acc: 0.9973 - val_loss: 0.4886 - val_acc: 0.9685\n",
      "Epoch 60/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0492 - acc: 0.9932 - val_loss: 0.6121 - val_acc: 0.9582\n",
      "Epoch 61/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0552 - acc: 0.9915 - val_loss: 0.3507 - val_acc: 0.9670\n",
      "Epoch 62/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0234 - acc: 0.9955 - val_loss: 0.2925 - val_acc: 0.9735\n",
      "Epoch 63/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0258 - acc: 0.9961 - val_loss: 0.3144 - val_acc: 0.9684\n",
      "Epoch 64/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0425 - acc: 0.9928 - val_loss: 0.4630 - val_acc: 0.9665\n",
      "Epoch 65/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0289 - acc: 0.9943 - val_loss: 0.3833 - val_acc: 0.9665\n",
      "Epoch 66/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0316 - acc: 0.9953 - val_loss: 0.4546 - val_acc: 0.9667\n",
      "Epoch 67/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0282 - acc: 0.9953 - val_loss: 0.3708 - val_acc: 0.9718\n",
      "Epoch 68/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0206 - acc: 0.9960 - val_loss: 0.4270 - val_acc: 0.9675\n",
      "Epoch 69/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0180 - acc: 0.9961 - val_loss: 0.3775 - val_acc: 0.9704\n",
      "Epoch 70/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0151 - acc: 0.9966 - val_loss: 0.4817 - val_acc: 0.9687\n",
      "Epoch 71/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0250 - acc: 0.9955 - val_loss: 0.5301 - val_acc: 0.9653\n",
      "Epoch 72/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0565 - acc: 0.9928 - val_loss: 0.4643 - val_acc: 0.9588\n",
      "Epoch 73/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0517 - acc: 0.9917 - val_loss: 0.5152 - val_acc: 0.9614\n",
      "Epoch 74/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0646 - acc: 0.9912 - val_loss: 0.4022 - val_acc: 0.9694\n",
      "Epoch 75/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0261 - acc: 0.9959 - val_loss: 0.5615 - val_acc: 0.9680\n",
      "Epoch 76/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0134 - acc: 0.9969 - val_loss: 0.4719 - val_acc: 0.9673\n",
      "Epoch 77/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0129 - acc: 0.9977 - val_loss: 0.4228 - val_acc: 0.9697\n",
      "Epoch 78/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0376 - acc: 0.9948 - val_loss: 0.5754 - val_acc: 0.9668\n",
      "Epoch 79/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0458 - acc: 0.9936 - val_loss: 0.5116 - val_acc: 0.9685\n",
      "Epoch 80/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0332 - acc: 0.9949 - val_loss: 0.4445 - val_acc: 0.9685\n",
      "Epoch 81/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0290 - acc: 0.9958 - val_loss: 0.5689 - val_acc: 0.9646\n",
      "Epoch 82/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0491 - acc: 0.9929 - val_loss: 0.5058 - val_acc: 0.9680\n",
      "Epoch 83/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0461 - acc: 0.9945 - val_loss: 0.4820 - val_acc: 0.9677\n",
      "Epoch 84/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0203 - acc: 0.9954 - val_loss: 0.4440 - val_acc: 0.9685\n",
      "Epoch 85/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0078 - acc: 0.9979 - val_loss: 0.5342 - val_acc: 0.9682\n",
      "Epoch 86/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0134 - acc: 0.9977 - val_loss: 0.5018 - val_acc: 0.9668\n",
      "Epoch 87/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0364 - acc: 0.9946 - val_loss: 0.6408 - val_acc: 0.9682\n",
      "Epoch 88/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0470 - acc: 0.9939 - val_loss: 0.5157 - val_acc: 0.9684\n",
      "Epoch 89/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0289 - acc: 0.9952 - val_loss: 0.4833 - val_acc: 0.9677\n",
      "Epoch 90/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0160 - acc: 0.9963 - val_loss: 0.4624 - val_acc: 0.9702\n",
      "Epoch 91/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0359 - acc: 0.9946 - val_loss: 0.4753 - val_acc: 0.9656\n",
      "Epoch 92/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0232 - acc: 0.9960 - val_loss: 0.4891 - val_acc: 0.9714\n",
      "Epoch 93/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0182 - acc: 0.9965 - val_loss: 0.4738 - val_acc: 0.9707\n",
      "Epoch 94/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0290 - acc: 0.9957 - val_loss: 0.4196 - val_acc: 0.9670\n",
      "Epoch 95/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0088 - acc: 0.9974 - val_loss: 0.4951 - val_acc: 0.9696\n",
      "Epoch 96/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0171 - acc: 0.9970 - val_loss: 0.5680 - val_acc: 0.9684\n",
      "Epoch 97/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0258 - acc: 0.9953 - val_loss: 0.4530 - val_acc: 0.9665\n",
      "Epoch 98/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0445 - acc: 0.9933 - val_loss: 0.3704 - val_acc: 0.9607\n",
      "Epoch 99/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0170 - acc: 0.9960 - val_loss: 0.4340 - val_acc: 0.9687\n",
      "Epoch 100/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0266 - acc: 0.9952 - val_loss: 0.5961 - val_acc: 0.9675\n",
      "394/394 [==============================] - 0s 576us/step - loss: 0.7295 - acc: 0.9674\n",
      "[0.7294687628746033, 0.967380940914154]\n"
     ]
    }
   ],
   "source": [
    "# MNIST를 이용한 Deep Network으로 구현해 보아요\n",
    "\n",
    "# Raw Data Loading\n",
    "df = pd.read_csv(r\"C:\\Users\\lee_0\\Desktop\\코딩\\ML\\12.07\\train.csv\")\n",
    "\n",
    "# 결측치와 이상치가 없어요!\n",
    "# Feature Enginerring을 할게 없어요!\n",
    "\n",
    "# 독립변수(feature), 종속변수(target) 분리\n",
    "x_data = df.drop('label', axis=1, inplace=False).values\n",
    "t_data = df['label'].values  # 원래 one-hot 처리를 해야 해요!\n",
    "                                        # 하지만 keras에게 one-hot처리를 위임할 수 있어서\n",
    "                                        # 따로 처리는 안할꺼예요!\n",
    "\n",
    "# 정규화는 진행해야 해요!\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(x_data)\n",
    "x_data_norm = scaler.transform(x_data)\n",
    "\n",
    "# train, test 분리\n",
    "x_data_train_norm, x_data_test_norm, t_data_train, t_data_test = \\\n",
    "train_test_split(x_data_norm,\n",
    "                 t_data,\n",
    "                 test_size=0.3,\n",
    "                 stratify=t_data,\n",
    "                 random_state=0)\n",
    "\n",
    "# Model 구현(Regression Model 구현)\n",
    "model = Sequential()\n",
    "\n",
    "# Input Layer\n",
    "model.add(Flatten(input_shape=(784,)))\n",
    "\n",
    "# Hidden Layer\n",
    "model.add(Dense(units=256,\n",
    "                activation='relu'))\n",
    "model.add(Dense(units=128,\n",
    "                activation='relu'))\n",
    "\n",
    "# Output Layer\n",
    "model.add(Dense(units=10,\n",
    "                activation='softmax'))\n",
    "model.compile(optimizer=Adam(learning_rate=1e-2),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['acc'])\n",
    "\n",
    "# model.summary()\n",
    "\n",
    "# Model 학습\n",
    "history = model.fit(x_data_train_norm,\n",
    "                    t_data_train,\n",
    "                    epochs=100,\n",
    "                    batch_size=100,\n",
    "                    validation_split=0.2,\n",
    "                    verbose=1)\n",
    "\n",
    "# Model 평가\n",
    "print(model.evaluate(x_data_test_norm,\n",
    "                    t_data_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAABGxUlEQVR4nO2deXwU9f3/X28SjnAJSLhvCfdNBFTEW6EWxINLrVSrtrXUqx5YW22tVtSfR1H0W2/Fg1LrgYqoRUWlgga57wARAgIhIdxHjvfvj/d+nNnZmd3ZZDeb3byfj0cem52ZnfnMTvKa97w+78/7Q8wMRVEUJfmplegGKIqiKLFBBV1RFCVFUEFXFEVJEVTQFUVRUgQVdEVRlBQhPVEHbt68OXfq1ClRh1cURUlKlixZsoeZM93WJUzQO3XqhJycnEQdXlEUJSkhoh+81qnloiiKkiKooCuKoqQIKuiKoigpggq6oihKiqCCriiKkiKooCuKoqQIKuiKoigpggq6oigpxezZwJ49iW5FYlBBVxQlZSgsBCZMAF5+OdEtSQwq6IqipAwmMt+9O7HtSBQq6IqipAxFRfKqlouiKEqSU1goryroiqIoSY5G6IqiKCmCEfSCgsS2I1GooCuKkjJohK4oipIiGEEvLgZKShLalISggq4o1YCXXgJ27Up0K5IfI+jO32sKKuiKkmB27gSuuQaYNSvRLUl+7CJeE20XX4JORCOJaD0R5RLRVI9txhPRGiJaTURvxLaZipK6mA68Q4cS245UoLAQqFdPfldBd4GI0gDMADAKQC8Ak4iol2ObLAB3ATiNmXsDuDn2TVWU1MQI+uHDiW1HKlBUBHTtKr+roLszBEAuM29m5uMAZgG4yLHNdQBmMPNeAGDmGjrwVlGixwiPCnrlKSoCuneX31XQ3WkLYJvtfX5gmZ1uALoR0UIiWkREI912RETXE1EOEeUU1NREUUVxYP4VjhxJbDuSnbIyyW7JypL31VFiDh8Gbr0V+Oab+Ow/Vp2i6QCyAJwJYBKA54ioiXMjZn6WmbOZOTszMzNGh1aU5EYj9NhQXCyvrVsDjRtXzwh9zx7g8ceBNWvis38/gr4dQHvb+3aBZXbyAcxh5hJm3gJgA0TgFUWJgHroscFkuDRrBjRvXj0Ffd8+eT3hhPjs34+gfwcgi4g6E1EdABMBzHFs8y4kOgcRNYdYMJtj10xFSV2M8KjlUjmSQdDNU0STJvHZf0RBZ+ZSAFMAfAxgLYDZzLyaiO4jojGBzT4GUEhEawB8DuB2Zi6MT5MVJbVQyyU2mEqL1VnQ4x2hp/vZiJnnApjrWHaP7XcGcGvgR1GUKFDLJTY4I/TVqxPbHjcSHqErihJf1HKJDclguVQHD11RlDjBnPoR+nXXyTyf8cYIepMmQGamjLytbjdJE6En1HJRFCU+HDhgVQVMVUFfsQL44Yf4H6eoSMQ8PV0idECi9Pbtw36sStm3T0oT1K0bn/1rhK4oCcRE540bV79oMlbs3SuVJE106sbLLwM9egDl5RU/TlGR2C1AsKDHkw0bgKuv9l+qt7g4fv45oIKuKAnFCE6HDqkboRshX7/ee5slS2T91q0VP04iBP3tt+VmtNlnkrYKuqKkMHZBP3ZMhq8nE3/5C/DVV97rmSVCB8ILuvke1q2reFsKC6te0DdtktfdPqtX7dsXP/8cUEFXlIRiLJcOHeT16NHEtSVamIG//Q2YOdN7m8OHgdJS+T2cWJvvoTKCnogI3UTmfgVdI3RFSWHsETqQXLbLwYPieW/b5r2Nic6B+EfodkFv2hQg0ghdUZQqpKAAqFMHaNlS3idTx6jxxvPzI2+Tnh5erCsr6OXlcvM48UR5n5Ym4h5PQT9+3LqZaYSuKAr27BF7oH59eZ9MEboZJBNO0E2E3r8/sHGjZb/YYa68oO/bJ/sxETog32s8S+j+8IOVlaMRuqIo2LNHBsEko6Cb6Lu4WOyXcNsMGyapfXl5odscPCgdwpmZkt5ot2n8Yh8lasjMjG+Ebs9s8SPox45JH4lG6IqSohQUSCSZkSHvk9FyAYDtzoLaAYw4Dxsmr24RuBHd4cPlNZzX7oWboMd7+L/xz7t29Sfo8R72D6igK0pCSWbLxS7oXraLPUIH3MXaKegVsV3slRYNVSHoGRliJ/kR9HgX5gJU0BUloRQUJL/lAngLuonQO3WS8wwXoQ8ZAtSu7V/Q7dk14SJ0Zn/7i5bNm4EuXaRDWyN0RanhlJSIKCar5WIECggv6I0aSZZL9+7uEbrpuGzVSuYD9SPoa9ZIqudHH8l7L0EvKZF6OfFg0yYR9BYt5PhuHb52NEJXlBTG2ATVJULPywP69fNfSKu4WG5EzZt756IXF0tOOCC1WsJF6M2be2/jxGzz73/Lq5eg2/cfS5glQj/pJBF0P8eJd6VFQAVdURKGXchMhJ5IQZ83D1i5MvxQfjvFxSJO7duHj9BNRNq9u0TjRnwNe/ZIBH/CCSLomzZFLnZljvfBB1IuoahICpyl2+rHxlPQd++W8rwmQjfLwmGeaDRCV5QUxAiNPUJPpOWSkyOvGzb4294MkmnXLnynqD1CB0JtF5PpQyTblJZGLnZljldQACxeHDxK1GAEPR656KZ99gg9kqBrhK4oKYwRmuoSoRtB95s26EfQnRG62/5Npg9giX4k2yU/X4Q0PR2YMye4MJchM9Paf6wxKYvRCPq+fUCtWkDDhrFvj0EFXVEShN1yqVVLJj1IlKAfOQKsWiW/u0Xoa9eG2iD79lmCXljo/nRhj9A7d3bPYrELuhF9P4Leowdw5pnA+++Hj9DjJehEkr0TTYR+wglyreOFCrqiJAh7hA6I7ZIoy2X5cvGiO3cWQben+m3bBvTtC8yaFfwZe4QOuA8uskfo6ekyCMctQjfRdOPGQJs2/gS9XTtg9GjJeFm1KlTQGzWS/VWm4JcXmzfL8evWtWZJ8hOhx9NuAVTQFSVh7Nkj/+C1a8v7+vUTF6Ebu+Xyy6UNdnH+7jsRe+ewfaegO22X0lIZ1m8idMA9i8V46OG2sVNeLu0zgg7IcZyCTgRccAHw4YeVmwnJDZOyaI6TmekvQo9nhyjgU9CJaCQRrSeiXCKa6rL+l0RUQETLAj/Xxr6pipJamEFFhoyMxAp6q1ZiYQDBtsvSpfK6a5e1jNmyELwE3S3vuls3EUMzkYfJUHETdK8BQXv2SKXDdu3kiaJvX1luKi3aGTMG+PFHmREplpiURUOLFkkSoRNRGoAZAEYB6AVgEhH1ctn0X8w8IPDzfIzbqSgph907BhJrueTkANnZlodtF/Tvv5dXu2AdOSKeepMmQNu2ssyZi24E3R6hZ2XJ50yu+969Itz276FvX/ns6tXubTU3DnMjGTNGXp0ROgD87GdSSnfOHPd9VYTDh+UmYSJ0wJ+gV5cIfQiAXGbezMzHAcwCcFF8m6UoqY/dOwYSZ7kcPCidntnZIs7167tH6HbBsudUN2ggou2M0M2wf2eEDlj7N/0I9u/hssvEm/6//3Nvr1PQLwqoUatWods2awacdlpsBX3LFnlNyggdQFsA9ntvfmCZk0uJaAURvUVE7d12RETXE1EOEeUUxLNQsaIkAU7vOCMjMRH60qXiMWdnSwZGVpbVcblzp0SjQLDl4rRT3FIXvSJ0QGqjA8GZPobmzYHx44FXX3Uftu8U9JNPBj7/HLj0UvfzGzMGWLHC/wjYSNhTFg3JFKH74X0AnZi5H4BPAbzithEzP8vM2cycnWm/JStKDcNM6lAdInTTITp4sLx262ZF0CY679s3WLD8CLpbhN6ypWSfmP27CToA3HCDiPnrr4e2Nz9fskpMuiAg3n/duu7nZyyZ9993Xx8tZlCR03I5dMj7+pWXA/v3V48IfTsAe8TdLrDsJ5i5kJmPBd4+D2BwbJqnKKmJmdTB6aEnStDbtbMsi+7dxVY4ftzyz88/XzovTS66c9Sj3widSKL0cBE6AAwdCgwcCDz9dGjnaH6+pDampfk7v6ws6Wj1Y7uUl8t5h2PzZkmHtHv25ubiZTwcOCDnUR0i9O8AZBFRZyKqA2AigKCvhoha296OAbA2dk1UlNTDmYMOJM5yMR2ihm7dJPtk82aJ0Lt2lR/AarezLkn79hLBHztm7cctQjf7d3roTkEnkih95Upg4cLgdSYHPRrGjAG++CK4QqSTsjK5cQ0ZIjMLebFli2TXEFnLIg0uqoph/4APQWfmUgBTAHwMEerZzLyaiO4josDDDG4kotVEtBzAjQB+Ga8GK0oqYDJC2tuefRMRoe/bJ+LqFHRAln//vUTKTsFys1wAYMcOaz/FxZJjb+rU2Pf/ww8i/nv2SKeqKX1gZ9IkEcCnnw5eXlFBLykBPv7Ye5sHHwTmz5dBVg884L2dqYNuJ5KgV0VhLsCnh87Mc5m5GzOfxMwPBJbdw8xzAr/fxcy9mbk/M5/FzHEYm6UoqYMZpNOpk7UsEXnoxlJxE/TFiyUaHTRIvG8gsqDbbRczStQeyQJigZSXizA6UzftNGgATJ4MvPWWVWqYuWKCPmyYiO6rr7qvX7QI+MtfgIkTgV/8Apg2TZ4OnDBbEbqdpInQFUWJPVu2iNA5I/QjR+I3w45XOwAr/xwQzzszE5g9W94PGmQJlsl0KS4G6tQB6tWT916CbvfPDeaGsXFjaMewk0mTJLKeP9867pEj0Qt6Whrwu9/JqFGnUB84AFxxhezzmWeAxx+Xdl97rTUAyrBrlxzfGaGbc0iKCF1RlNiSlyc53/bMjPr1RUAi1QKPJTt3yqszh7tbNyA3V373slzs0bcRWPvgIq80PZO6uGFDaOqmk+xs6YD87DN570xZjIYpU6TS4UMPWcuYRejz8oDXXpP2nngiMH068O238mrH3ACdEXqDBvKjEbqi1EDy8oLtFiAxJXR37hQRM5G2wUTs7dpJ9Nm4sdx8jGA5B8mYQlh2QfeK0Js2FRHfsCG85QJIeuIZZ1gRemUEvVkz4Ne/liJjRphfeAGYORO45x5rkmoAmDBBOkgfeij4iclL0IHwuegaoSspwbJlsRvQkUps2RIq6ImY5OLHH91HWBpbZOBAeSUSwbJbLk5xysqSEaeGcANpTOpiJEEHgLPPlqeFrVsrJ+gAcOutYr888ohk8EyZIsL9pz8Fb0cEXHihnK95igGsHHTntQPCF+jSCF1JCSZMAP7850S3onpRUiLC5IzyEjGv6M6d4QV90CBrmT0CdRPrwYOlCJaJaL0idLP/lSslHz/SGMNzzpHXzz6T761WLfc2+6FNG+loffFF4JJL5Nivveae096vn7yuWGEt27IFaN3aPSsnUoSekSH9DvFEBV2JK7t2xWcKsGQmP1+yPCpjudx8MzA1pO5p9HgJ+uDBYrGcfba1LJKgZ2fL8s2brWqMXhF6t25W5kqkCL13bxFeI+itWlklhyvCHXdYN9XZs71vKEbQly+3lm3e7G63AOEFvSqG/QMq6EocKS2VyMQ8biqC8WErY7nMnw/MnVv5tuzcKRGnkw4dJPtjxAhrWcuWkSN0QKL0Q4fk+ntF6KZjFIgs6LVqyY3ls8/Eo6+o3WLo2hV48kngX/8CTjnFe7tmzeRYzgjdmeFiMILulqVUFYW5ABV0JY4YIQ83Oq8m4paDDkRnuezdK/txisfu3f5n6Dl4UH687AtnFGw8dGZr+jk7ffqIpZCT414L3Y6xdIDIgg6IoG/fLvnilRV0QEahXnJJ5O3697ci9JISuaGEi9BLS90DGI3QlaTHPFJrhB5MXp5Ene0dNUmjsVz27pUI2gyvN9x5p9QA94NXyqIXLVpInZOCAnmKcEacdeqITbFkidUurwjdlBIAInvogGX9HDgQG0H3S79+coM8dkw6ZcvLw0foQHBVSoOZDCTeqKArcaOoSF41Qg9myxYRJWcE7NdyOX7cEn1nBtGaNXLDiFRgCohe0M1oUVNYyy3iHDxYRp961XExNGhgTYzhJ0I/6SSxgYCqFfT+/SXqXrs2fMoiIB2ugPvcqm5PNPFABV2JG0bQDx+u2sEy1R23HHTAv+Vij8qd83xu2iSWiJuoOKlIhA5YtdK9BL242JryzStCB8RHJ3KfacgJkRWlV3WEDoiPblIWvQS9Y0d53bo1dJ1G6ErSYwQd0CjdTl6euygYyyVShO4l6MXFls3lJipOjKC7dYq6YQTdVEp0E3RTE+a//5XXcII+YIDYTn7L4J57rrwa4awKsrJk0NXy5RKh165tPVk4addObjxu4y6qKkJPj/8hlJqKU9D9PFqnOseOSfQcqwjdLh5mJh3An6D/+KOIqdvkym4YQQ9nufTuLV76ggXe2xjuuw+45RZ/xwZkTEPduuEzU2JNerqc0/Ll8vfbsaP3DahOHbk5Or/7o0flumuEriQ1GqGHsm2bWCJugu63U9QrQo9W0HfuFF+8lk8VMJ2XxnJxEyjTMXrokPc2hkaNLF/cD+npMt+os3pjvDGZLuFy0A0dOoRG6FU17B9QQVfiiF3QNdNFMALsJgx16oi4+rVcunYNFnRTTKtRI/+CHs2Iy9q1xe82x/ESKGO7NGokIpzs9OsnJQpWrPDOcDF07Bj63ZvrpRG6ktRohB6KVw46IJGnn0kujEAMHBhqubRqJTne8RB0QGwXMyuRl6CbAUbh/PNkon9/eT12zF+EbtIbDeaaR/M0UlFU0JW4UVRk+eYq6MKWLeLBenWs+Znkwgj6gAHy5GO+29xcidqNqNhZuhS47bbggUheo0TDYVIX09Ik9dANI+hVYTFUBSbTBfAXoR8/HlwCwAz06tEj9m1zooKuxI2iIiuiUctFyMuTzA4vK8JMchGOvXtFTM3weROlb9ok+drmsd8u3i+9BDz6qJV6V1YmA2AqEqED7jMRGXr3ls7LVInQTQkAwF+EDgQ/Oa1bJ/uoiqQAFXQlbhQVWdaCRuiCV8qiwa/l0rSp9d3m5clNYPt2awDOwYPBN1EzS4/JDy8sFFGvjKB7UaeOVEjs3Tu6fVdnjO0SSdDdctHXr5f68lXRmauCrsSNoiIRgIYNVdANbnXQ7fi1XJo2tcQjL8+KvI3lAliiwhwq6NEOKjIYyyVSB9/77wMzZkS37+rMuedK30SkQVBeEXpV2C2ACroSJ8rLRXiaNZN//mS2XI4cic0N6ehRyf0OJ+h+LRcz72dGhoiHSVm0D5E3gr5zpzXgqLKC7idCB/ynQiYLN98skXakKLtJE5m5yXz3+/bJd62CriQ1+/ZJZNismfyRJ3OEPmECMHRo6ITB0WL+yStruRQXi6ATyc0hL89KJbQLuokSTXSelSV1VpijHyVq8CvoNRl7LrrJ2bdPwh1PfAk6EY0kovVElEtEnmX1iehSImIiyo5dE5VkxKQsJnuE/v33Yh+sXw988EHl9hVu+jJDNJaL2VdenkToTZrI952ZKZ2S5gZiBH3yZPnsli3ypABYFopfVNAj07GjJehVmeEC+BB0IkoDMAPAKAC9AEwiol4u2zUCcBOAxbFupJJ82AU90RF6YaFkKbz8cvSfffBBuSG1awf84x+Va4eJou2lY51EY7kAlniYDBciqzSvXdBbtQIuuEDeL1kiEXrDhvITDeYGoILujT1tdP16yWiKlO4YK/xE6EMA5DLzZmY+DmAWgItctvsbgIcAHI1h+5QkxRmhJ1LQH31UMkBeeCG6z61dC/znPzKR8I03Ap9/Hjx7TbRs2iSCHc63jmS5lJRIBos9Qi8slMm4TzrJ2s4uKitXAn37yk/t2pagV2ReThOh+6mQWFPp2NGqV79unVyXykyZFw1+BL0tgG229/mBZT9BRIMAtGfmD8PtiIiuJ6IcIsop0IkmU5rqYrns2QNMny5CuXBh8AzukXjoIam0d9NNwLXXyj4qE6Xbo2gvMjLCR+jme7QLOiA55fbI3wh6WZnUSO/bV2yYPn0qJ+iNG8s8nL/6VfSfrSnYO6WrMsMFiEGnKBHVAvAYgD9E2paZn2XmbGbOzvQzTYmStLhZLm5zLcabRx6RiPeVV+T477zj73N5eTIb/PXXiyfdtClw1VXA669XfNLr3NzgKNqNSBG6cyYgeylZZ4S+Y4c8ZRw9KoIOyCjOJUvEQ6+IoAPAuHEV/2xNwFyTzZvlmldVhyjgT9C3A7BPltUusMzQCEAfAF8QUR6AYQDmaMdozcYIetOmEqGXlIiwVCW7dwNPPQVMmgRceqn8Y731lvf2zBJRPfsscOWV4kXfdpu1/sYbpZ7HP/8Z/rgLFsg/tUkVBCSNc/Pm8P45IBH60aPBtUDsOAXd3sFqF/SOHeV85s2T9336yOvgwbKPjRujz3BR/GEi9AULpAxAdYvQvwOQRUSdiagOgIkA5piVzLyPmZszcydm7gRgEYAxzJwTlxYrSUFRkVTbq13bGoQSL9vFS/weeUTE8Z57xOa49FL5J9uzx337sWOBnj2BX/9a7JGHHw6eHadnT+lY/Oc/wz9tvP22PG5/+621bPt2uRn4idAB75ufU9BbthRbCAi1XADgww/l3HsF0hhMnZXyco2y40Xr1tIR+skn8r5aCTozlwKYAuBjAGsBzGbm1UR0HxGNiXcDleSkqMjqODMZEfHoGF24UG4Yzpnu9++XkYqXX2498l52mXjK770Xup9jxySaHTdOZuTZsUMGkzi5+GIgPz+49rhbmwDpqDTYB/6EI9IkF05BJxLxrlcvOOI2gv711yL0Zr99+1p1ZFTQ40NamgQCJl20ulkuYOa5zNyNmU9i5gcCy+5h5jku256p0bliF3QTocdD0O+9V7I+jIgali+XzsXLL7eWDRggg3rcbJdly+TxeMIEa65LN04/XV6//NJ9/cGDlpAvX24tN4Lux3IB/As6IO3t3j14dGb7gElaWmr554AIv7FfVNDjh/HRMzOrNiNIR4oqccFN0GNtuXzzDTB/vvy+alXwOvPeLmbGdpk/P7QtixbJ67Bh4Y/Zs6dUzfMS9G+/laeAJk2CBT03VyLj9u3dP2cwkbRXposRdHse+FNPAbNmBW+XkWHNMGT/DgDLdlFBjx9G0KvSbgFU0JU4UVHLpbRU8nf98MADMh9mz57A6tXB61atkhQ7Z93xyy6TDto5jmfLxYvlMdmrTrmBSKJ0L0H/+mvZZvJksW5MpL1pkzwdRJrBx4/lkpEhKYiGTp3chcPYLk5BP+MM+XxVTrZc0zDffVXaLYAKuuJCQQFw3XWRh6CHo6IR+oMPBk8o4MXSpdLhd/PNwJAh7oLep0+odTJkiAj3228HL1+0SOq1+GHECBk+n58fum7hQikbe+aZ0vFonhRMDnok/FgufuuMewn6lVdK+/1ODq1Ej0boSrVh3jzg+eeBnAr2hDBXPEJfskRywA8eDL/d3/8uEfiUKSKgO3ZYdgSzCLzxiu0QSTbLxx9bExnv3i0CF8luMYwYIa9ffRW8vKxMbKDhw6362cuWSXv85KAD/iwXv4Levbt8987jEmnKYrwxQ/17hRRJiS8q6EoIZsi4fRqtaDh4UKwTI+gNGkjPvx9BNwWsduzw3mbdOhmS//vfi2CZiRRMlL5rl+SAuwk6IJkqR4+KqANitwD+Bb1/f0nJdNouq1aJXXTaaWKDNG4sPnphoWTdROoQBfxZLn4F/Y9/lJtyWpq/7ZXYceaZ8hRo6udUFSroSghG0Cs6ItI+ShSQiLBx48iWC7M/QX/tNdnnjTfKeyPcxt4wr14z5owYIW0zo0YXLRJve9Cg8O0zpKWJaDsF3WTanHaatK9/f4nQ7aVtIxFLy6VRI3/HVGJPrVoSOFR1XXgVdCWEWAs64K/iYkGBZYNs3+693TvviCibQlHt24t4mQjdCLpXhJ6eDoweLeVwS0pE0Pv1s6JjP4wYITVS7IOUvv4aaNPGGr3Zv78U84pG0GNpuSg1DxV0JYR4CLqfiosmOge8BX39ehHSSy6xlhFJNG4EffVqSdkzgu/GxRfLE8Nnn0mqoV+7xWB89K+/tpYtXGhF54DkvR88KCMGifyVUI2l5aLUPFTQlSCY4yfokSwXu6B7WS7GJhk7Nnh5797BlkukCYrPP1/E84EHRHSjFfTsbBmkY2yX/Hz53k47zdrGdIy+/76kQ5oh+uEIZ7mYlE4VdMULFXQliOJiK8Okqi0XI+gdOnhH6O+8I2LqHKDTp4+0d/duK2UxHBkZwMiRVqZKtIJet6585r33gLvussrJ2gW9d2/x24uL/XWImnYBlvVkx1k6V1GcqKArQZjoPD294lku9kqLBr+WS5s24jW7Rejbt4s9cvHFoetMRP7RR3JDiiTogLWfZs38C66dUaOkzY8+KoOIxo4Vm8WQkWENLPHbOZmWJt/Bli2h69yG/SuKHRV0JQgj6L17Vy5Cz8iwok3Av+XSpYvYE24R+rvvyqvdPzcYAf/Xv+Q1kuUCABdeKDeuoUMjz+buxu23S6fo0aMiwO+8EzoS1Ngu0WSb2PsD7KigK5FQQVeCMJPbZmeLWHmVpg2HfVCRoUkTycUOtz8j6G3aSITu3Pbtt2Xkndvou1atROg+/VTe+xH0pk2B554D/vznyNu6QSSjLcOlppmIPZongN69pePXef4q6EokVNCVILZuFX+4d28Z+ViRglpugn7CCdLh6jUC9Ngx6Vg0EXpJSfAEEYWFUsvczW4BrEyX0lL5vF/R++UvgVNO8bdtRTjrLPk+TUEsP/TpI2mLTttFBV2JhAq6EsTWrdLhaFL+KmK7eEXogPcN4ocfRPCNoAPBtsu8eXKD8RJ0wLJd/ETnVcXJJ0sHZzSzvjtHvhpU0JVIqKArQWzdKlkmpvRqRTpGvSJ0wLtj1GS4GMsFCO4YXbJEPPlwozmNEPrpEK1Koh16b+p/OEsCq6ArkVBBV4LYulUqxRlBDxehr18vFoeTcILuFaHbBd0tQl+50koD9KI6RugVoXFjuam6Rej16vnLZ1dqJiroyk+UlEhU3KFDZMtlzx4py/rMM8HLi4tlH/bJi4HIFRc3bxahatXKqgRoF/QVK0LLwDoZPlzmER03Lvx2yYBbpouOElUioYKu/MT27eJjd+ggs/IA3oK+bp3cAL74Inj5d9/Jq3Ogjh/LpUsX6dysXVtuKMZy2b1bfiIJeno6cNttUtcl2endG1i7NvgJSAVdiYQKuvITJge9QwfJzGjc2FvQTcGp//1PbgKGRYtElE8+OXh7P5aLvePQnotuJtuNJOipRO/eMsepfTJqFXQlEiroyk/YBR0QH92rU9QI+s6dVu46IILeq5cl4IZwEbopm+sUdBOh10RBN/0BdtuluFgFXQlP8gn6oUPy328PC5WYYATd1EnJzAwfodeuLb//73/yyiyC7lYXpV49ifrdBL2wUIpO2QW9TZvgCL1FC6Bly+jPKVnp2VNeTabLzp0i7hUpUaDUHJJP0J98UsZRHz2a6JakHD/8ICJuhuy3aOEt6Bs3SgnZhg1l2jVARL6oyLvQldfwf3uGi6FtWzn2sWP+OkRTjQYNZFJpE6E/9ZT0WdxwQ2LbpVRvfAk6EY0kovVElEtEU13W/4aIVhLRMiL6mojiN5NepBEqSoUxOegGrwjdzJHZo4fUQTER+qJF8uol6F4VF90E3eSib98uolbTBB0Q22X1ankofeYZ4KKLgKysRLdKqc5EFHQiSgMwA8AoAL0ATHIR7DeYuS8zDwDwMIDHYt3Qn4hmxmElKrwE3elu7dkjdVmysmTY/PLlMqR/0SLJMDF2gZNmzcQ6cGIE3Z7qaHLRv/5ahsHXREHv3Vty/Z97Tp58/vCHRLdIqe74idCHAMhl5s3MfBzALAAX2Tdg5v22tw0AxM/gjpQuoVQIM7GFU9BLS0O/6o0b5bVrV+DUU2VI/nffiaAPGeI9+GfYMJmQ2Tm92oYN4o83aGAtM4I+b5681lRBLy0F7r1XnoTstdYVxQ0/gt4WwDbb+/zAsiCI6HdEtAkSod/otiMiup6Icogop6CitVnVcokLZmILp6ADobaLyXDp2tWyV+bPl0g93EQR558vXR/2advKy2WKNjOlm8FYLmb6tmQf/VkRTKbL/v0SnVekxK9Ss4hZpygzz2DmkwDcCeBPHts8y8zZzJydadQiWlTQ44IzZRHwHi2amyslYzt3ljS6nj2BZ5+VSD2coI8YAdSpIyJtWLIE+PFHYMyY4G1PPFGyYgoL5cYRzQTOqUKPHvI9d+oUviiZohj8CPp2APYJv9oFlnkxC8DYSrQpPOqhe7JsmfxUBCPoHTtay8JF6B07ijgDYruYbYYO9T5GgwZiG9gFfc4cEa1Ro4K3JbKi9JpotwCS6nnXXcD06aETZyiKG34E/TsAWUTUmYjqAJgIYI59AyKy971fCGBj7JroQD10V5hlJp9f/rJin583TwTankXhJegbNwbnQ596qrx26WJ9xovzz5c0RNM5OmeO1GA58cTQbWu6oAPA/fcDo0cnuhVKshBR0Jm5FMAUAB8DWAtgNjOvJqL7iMg8KE8hotVEtAzArQAmx6vByMiQES0q6EF8841MiLBypfckEl4UFwOvvAJMmmQ9AAHuJXSZRdDtwm8E3c9Ey+efL6///a/kva9YEWq3GEzHaE0WdEWJBl8Pcsw8F8Bcx7J7bL/fFON2eUPkbwr5asaGDcBNN8mcl40bx37/r78ur+XlQE4OcOaZ/j/70kuS6/z73wcvr1dPBg7ZI/SiIvnq7RF6t27AhAnAZB+38QEDpPDXJ59Y92QVdEWJDck3UhTwN+NwNePTT8XWWLAg9vsuKQFmzwbOOUfemwE+figrk1GIp53mPk2ac7SoPcPFUKsWMGuWFX2Ho1Yt4LzzRNDfew/o3t17sMx558k+o5lgWVFqMskp6E2aJJ2gm07HaMTWL598IoN9brxRhHbxYv+f/egjGdjjjM4NztGi9hz0inL++cCuXWK7eEXngHSUfvxx9DP+KEpNRQW9ijCCHo3Y+uX112UU5siR4mMvWuS/dtn06dL5eMkl7uudgp6bK65XNHNkOjnvPOv3cIKuKEp0JKegn3BC0nnopsTst9+KzRErDh4U62L8eMlSGTpUMki2bYv82TVrxAq64QarcqITZwnd3FyrXnpFadtWSuyeeKKUDlAUJTYkp6AnaYTeoIGUiV27Nnb7ffdd4PBh4Ior5L3JNIlk7Xz9NXDuudLped113ts567nk5samhOv06cALL6idoiixRAW9CjBzdZp84ljaLq+/LoN8TOpgv36SneIU9EOH5OfwYeDxxyULpkEDYOFCa0SoGy1aSPv3B6r1OFMWK8o550j1QEVRYkfyCvrhw6I0ceCZZ6TDLlaYuTrPOUeGyjvF1ss9+uor7xmDALlJfPIJcOWVkj0CiO0yaFDwTeOWWyQSb9hQRPzWW8W7zsmRG0A4TC76jh3A3LmStqiTLChK9SQ5BxTb5zMzsxnHiIICyRY5+2yxJMJt58zH9sI+rH7o0GCxXbRIRkp+8IF0ahpyc6X2Sdu2wNtvSxVDJzNnSt65c3TosGHAjBkyJ+VXXwFPPAGMGyfzfJaXiwc+caK/Yk9G0AcNksJaDRoAp58e+XOKolQ9yRuhA3GxXWbPlpKlZh5LL265JThbIxz2wldDh8q0YgcOyLL775dO0pdeCm0HIJH3iBGh65ll2fDhoTeVoUNlpp9vvhF/vFs3GQl6++3AnXfKiFC/lfsGDpSfCRPEr9+92/3moihK4knOCD2Ogv7aa/L6449S6c+txgggpWLz8iRSj1S/xC7ow4aJGOfkyL4//FBsmPffl4yVhg1l29mzxRefM0fE9JprJNf89ttl/aJFMvnBHXeEHs90jF51lRx7wQJrWrloadUK+P77in1WUZSqJTkj9HBTyFeC3FwRyrPPlvdmgl4npaUylB8Ali6NvF/7XJ0mul20CJg2TQT8lVdk0oc5gZJn69fLDWP8eBH9efOAyy6Tyntm/s6XXpKSsuPGhR6vfXsR4q1bgd/+NrTWuKIoqUlyCnqcIvTXXxcr4sEH5b2X7bJli/jTgD9Bt88E1KyZWCCzZ0tdl9/+FrjwQqBdO+DNN2UbY7dcdpm8pqcDzz8vQn355fL0MGuWiHmjRqHHIwLOOkuOOW2av3NXFCX5UUEPwCx2y1lnSedh06beEbrJIyeKXtAB8biXLZPBPLfcIj75xIkyzL2oSAR9+HCrOBUgDyVvvikDhk47TTz4q6/2Pubzz0uUH49CYIqiVE9U0AN8+61YLldeKULdp493hG4E/ayzIgu621ydxuO++mqgdWv5fdIkycK8/365kYwfH7qvYcOAv/5VnhC6dAlvpdSvH1wKV1GU1Cc5Bb1hQ1HdGHror70mA3JMTZO+fUVY3WqirF0r9U/OOEMG2oSrP27m6rTPBDR6tKRE3nWXtWzgQBmw88QTcmrGbnEydaoM1X/4YZ1jUlGUYJJT0GvVimkJ3ePHxZMePdrqb+3TR0ZHutVEWbtW5nscOFAEf/ly7327zdXZvr3UULEvI5IonVkibxO5O0lLkxzzSy+N7hwVRUl9klPQgZgO/587V1IC7RM0mEkVnLYLswh6z54i6EB428UU5bKLtxeXXy6CbeqyKIqiREPyCnoMKy6+8grQsiVwwQXWsj595NUp6Dt2SIdkz57Sadm8efiJmd0idC+6d5fa5NdeG1XzFUVRACSzoMcoQi8okGH3v/hF8MzqTZpIKqEz08V0iPbsKTbJwIHhI/StW6XUbKTBR4YOHdQbVxSlYtR4QX/jDRko5DYfZt++oRG6XdABEfRVq7zrhG3dKp55reT9phVFSRKSV2ZiJOgvvyxzaRqLxU7fvsC6dcFivXatuD2tWsn7gQOlU3XNGvf9b90anOGiKIoSL5JX0GPgoS9fLv63s1qhoU8fEWszjyZgdYgaWyRSx6gzB11RFCVe+BJ0IhpJROuJKJeIprqsv5WI1hDRCiKaT0Txj0mbNJG8wvLyCu/ilVdktOakSe7r3TJdjKAbsrKkpKwR9L17rY5QM7GFCrqiKFVBREEnojQAMwCMAtALwCQi6uXYbCmAbGbuB+AtAA/HuqEhNGkiOYRmKp0oKSuTwURjxnhXVOzRQ9IITcfo3r0yW71d0GvVAvr3l47V88+XGX46dZKBP/n50kQVdEVRqgI/EfoQALnMvJmZjwOYBSBo8jBm/pyZDwfeLgLQLrbNdKGSFRcXL5YMF7ch9oZ69aSQ1nvviV3v7BA1nHKKpBvm5QG33SaDfu6809q3CrqiKFWBH0FvC8A+XjI/sMyLXwH4yG0FEV1PRDlElFNQUOC/lW5Usp7LvHkSXUeapGLaNOkYPftsmf0HCBX0++4Tn339eqnUOHs28Pe/A0uWyHrtFFUUpSqI6QQXRHQlgGwAZ7itZ+ZnATwLANnZ2S5VUqKgkoL+0UdS7Kpp0/DbjRkjEfoll0gdlbp1xVKxU79+8KxBRFKnpV8/qaDYpUuFmqgoihIVfiL07QDa2963CywLgojOBXA3gDHMfCw2zQuDsVwiCHphITBlijUEH5Bp1HJygFGj/B1q1CiJ6Bs2BHr1El/dDxdeCEyf7n97RVGUyuAnQv8OQBYRdYYI+UQAl9s3IKKBAP4JYCQzh5mnPjaUlwO1TIQexkMvK5NyuPPmSd/pq6/K8o8/lle/gg5IZcWcHBmEpCiKUh2JGKEzcymAKQA+BrAWwGxmXk1E9xHRmMBmjwBoCODfRLSMiObEq8HTp8usPyUNmsiCMBH6/feLmPfuLZNDmHTCefMkG8XkkPule3fZl6IoSnXEVx46M89l5m7MfBIzPxBYdg8zzwn8fi4zt2TmAYGfMeH3WHFOPFGC8vU7w1su8+bJZBBXXSUTMTNLrfGyMonQL7hAh+MripJaJJ2k/TTYZ226jOhxsVx275YStH36AM88I1kmEycCzz4rdcgLC6OzWxRFUZKBpBP0Hj2kKuKqVfCs5/LBBzI350svSQYKANxxB3DoEPCrX0kWSqR0RUVRlGQj6QS9Th3xsleuhKegL1gg5WoHDbKW9esHjBwpQ/GHDJE65oqiKKlE0gk6YJvA2WUaOmbgiy8kK8VZV/zOO+VV7RZFUVKRpBT0vn1lmP2Bhq1DPPS8PMlmOfPM0M+dcYYMErr55ipopKIoShWTtIIOAKvROyRC/+ILeXUTdCIZ+WnGJCmKoqQSSS3oK493cxX05s1lRKeiKEpNIikFvWNHyVhceaCzCDpbZWEWLJDoXOflVBSlppGUgl6rVqBjdG87GSl0WCr35uVJzRY3u0VRFCXVSUpBBwITOO9qAQaAnTsBhPfPFUVRUp2kFfQ+fYDCQ/WwCy1lbD/UP1cUpWaTtIL+U8dopzHAv/8NQPxzt/xzRVGUmkDyC3qPccDXX+OHxTuRl6d2i6IoNZekFfTMTKBlS2BlxhAcRgau+UUJatWSiZoVRVFqIjGdgq6q6dMHyNl4AkY3/Byfb2yLV2fKpM6Koig1kaSN0AGxXVatAj4/NASv4ipceWZ+opukKIqSMJJa0IcNk5z0Vx/aiSvxOvDWW4lukqIoSsJIakEfP14ms7jy9tZA//7A7NmJbpKSisyeDTz0UKJboSgRSWpBJ5Ip6QCIun/zDbBmTULbpKQgzz8v8xmWlCS6JYoSlqQW9CCuvlpGFU2Y8FMpAEWJCfn5wJEjwPffJ7olihKW1BH01q2B118HVq8Gfve7RLdGSSXyA53tX3+d2HYoSgRSR9ABSUL/05+Al18GXnwx0a1RUoH9+4EDB+T3r75KbFsUJQK+BJ2IRhLReiLKJaKpLutHENH3RFRKRJfFvplRcO+9wDnnSJS+enVCm6KkACY6b9xYInRbqWZFqW5EFHQiSgMwA8AoAL0ATCIiZ/mrrQB+CeCNWDcwatLSxHpp2BC49lopr6soFcUI+iWXAIWFwLp1iW2PooTBT4Q+BEAuM29m5uMAZgG4yL4BM+cx8woA5XFoY/S0bAk88QSwaBHw9NOJbo2SzBhBnzhRXtVHV6oxfgS9LYBttvf5gWVRQ0TXE1EOEeUUFBRUZBf+ufxyYNQo4K67ZNYLRakIRtDPPBNo0UJ9dKVaU6Wdosz8LDNnM3N2ZmZmfA9GBDzzjPz+m9+o96lUjPx8EfK6dYHhwzVCV6o1fgR9O4D2tvftAsuqPx07Ag8+CMybpyP9lIqRnw+0aye/n346sGULsD05/vyVmocfQf8OQBYRdSaiOgAmApgT32bFkBtuEP/zrrtU1JXo2b7dEvThw+VVo3SlmhJR0Jm5FMAUAB8DWAtgNjOvJqL7iGgMABDRyUSUD2AcgH8SUfXJF0xLA2bOBCZNAqZOVVFXosMeoQ8YINlT6qMr1RRf9dCZeS6AuY5l99h+/w5ixVRP0tOBV1+V36dOFT/05pv9fTY3V3KQW7SIW/OUasrhw0BRkSXo6enAKaeooCvVltQaKRoOI+qXXgrccgvw5pvWuu3bgSuuAH7xC4nmf/wRmDsXuOACICtLRqCWOzIyDxyonJe6Y4d21FZ3zPVtZ4tVTj8dWLkS2Ls3MW1SlDDUHEEHRNRfe01mkp48Gfj0U+C994B+/YB335XO06uuAtq0AS68UP5xJ0wAli8H/vMfaz/l5cB55wG9ekknWbR89BHQtq1U8IsXL74I3Hdf/PZfEzApi3ZBHzFCbsQLFyamTYoSDmZOyM/gwYM5Yezdy9y3L3OdOswA88CBzOvWMZeVMS9Zwvzww8xvvMF87BhzaSlzz57yU1oqn/+//5PP1a7NPGSIbBcNl1winwfkWLHm+HHmzEzmtDTmnTtjv/+awquvyjXasMFaduSI/N3cdlvi2qXUaADksIeu1qwI3dCkiUTjAwYAt90mddS7d5fpjwYNAm6/XTpR69SRTtW//AVYuxb4179kRo2pU4GzzgLeeAP49lspCOaXoiLg/feBG2+U7Js77gBmzIjt+X34IVBQIGUP3kh8NYakxUTobW3j6OrVA4YOBb78MjFtUpRweCl9vH8SGqFHS1mZRPRZWcxXXCGR+dq1su43v5Eobu5cf/uaMUO2X7pUIumLLpL38+fHrr0//zlz69bMgwcz9+8fu/3WNG64gblp09Dld98tTz/791d9mxJNWRnzP/7BvGlToltSY4FG6JWkVi3xuzdulMJft98O9Ogh6x57TGarnjBByvZG6uh89VXx7AcMAGrXBmbNklrusUqn3LFDOnQnT5ZJP5Yvlx8leuwpi3bOOEOefr75purblGgeeQS46Sb5u1eqHSrofhk7FhgyBOjSBbj7bmt5RgbwwQfAwIEioGPHAjt3uu9j/Xpg8WLpeDXUqwdMmQJ88gmwalXl2zlzpnTaXnONWDq1a1spm6kMs8z9OWqU3HhjgZegn3KKWHE1zXb58kv52ycCPv880a1R3PAK3eP9k1SWi+HAAebiYvd1ZWXMjz3GXLcu84knMs+axVxeHrzNH//IXKsW844dwcv37GHOyGC+5hrvY69bJ5254SgvZ+7Wjfn0061lF1/M3LIlc0lJ+M8mM199xTx0qNXR/Pvfx2a/LVowX3ed+7qhQ5mHD4/Ncezs2cN8773Mq1bFft/RkJcnHevjxzMvXy6d661by9/X3XfL9/zjj4ltYw0FYSwXFfRYs3atZL4AzJddxrx7tywvK2Nu35555Ej3z/32t5I94ZaVsmGD+PatWzN/9JH3sb/6So770kvWsnfekWUffljRM2IuKJD2e1FezvzWW4n5Bzfn16YN84svMl96KXPz5tI/URmOHpX9/vWv7utvv12u1+HDlTuOoayM+YUXJBgAREgTySOPSDsaNpTX1q2Z69UTcf/2W1n25puJbWMNRQW9qikpYZ42Tf7ha9eWjrXMzPD/BOvXy/p77gldN3q0/GP16iXb3HAD86FDwduUlTGPHSvbHTxoLT92TETi5z+PPr2SmXnhQmYi2cfYscyPP868b1/wNg88IO36+c+j339lKCpibtVK0k7N9/HBB9KWOXMqt+/Nm2U/L7zgvv7992X9559X7jjMcvM5+2zZ3/DhzKNGMTdqJDeVRHHaafK9FhUx/+UvzO3aMc+cKetKSpgbN/Z+eqksS5dK8sGzzzJv3165fYULRJIUFfREsWoV8x13iAVw/fXMt94a/p909GiJLu1R3yefyGWaNk1yoG+5Rd737Mm8YoVsc/w48+WXy/L77w/d7x//KOvatxdBPnDA/zmMH8/cpAnz1Vczd+ki+8nKkn86ZhE8s2+AOSfH+mx5uUSyY8YwT5kiOfcbN/o/9uLFzDff7B1tX3ONZJt8/7217PhxsUouu8z/cdz48ks5n48/dl+/d6/c6Lwi+Ioc64EH5DszN4t58yq/74qwc2fkcxs9mrlr1+j3/eKLkcdenHkm/2SfAcwjRoQGEX5Ys0ZuRA89FP1nK0t5ufyPOG3XGKCCnix8/rlckjFjxOYoKZGovEuX4BvBp5+KL16vHvNTT0lkDDD//e/uf0Dl5WK5jBgh27Vowfzaa5H/2HbsYE5PlxuRYcECsTfq1mW+8UYR1AsuEO+3aVNpu+GVV+R4Xbown3AC/2SN+En3M6miAPODD4au//RTWXfXXaHrbrpJno6KiiIfx4s33pD9r17tvc2AAcznnFPxYxj+/GfpWzF9JIcPM9evLzacFzNnSj9NPHjuOTn35cu9t3nsMdlm61Z/+zx+XM7HiPRzz7lvZ25ujz/OvHKlfDeADOaLhrw85rZt5bP164f2W8WbmTPl2O++G/Ndq6AnE489JmLUujXztdfKJXrnndDtdu5kPv98WU/E/PTT/va/cKHl8Z93HnNurve2f/0rh4yUZGbetcs69sknWxH/3/4my5YsYf7hB3ksHz7cGmFr7JubborcztmzZV+dO8vNY906a93+/cydOkkH3ZEjoZ9dsqRiImDn4YdlH+Eiw1tukWv16acVPw6zdLAOGxa87OKLRZDcbrpr1siNNi2N+X//q9yx3bjwQvnew93wly2T7+eVVyLvr7BQbnyAPLGdd55c0+++C932vPMk4DAWWnk5c58+8jfrl5075SmySRPmt98W2/P66/1/vrIcOybfHyCjwmOMCnqysWyZ5Zeffbb3P1ZZmYhWtH5xaalE9o0aMTdoIFGRk+PHJZq+4ALvY7/zTnAUXFws/0SjRzOfdZb4+c4BKL/9rUSjS5aEb1/PnvId5OfLPk8/XY65caNE7rVqubebWb6v3r2ZTz017NcQlptuku8nHLt2WSUk3G66figqknP585+Dl7/8slx/p+iVlzOfe658J506yY9X5lVF2L9fxPaWW8JvV1Ym/Sq//GXkfY4cKaJqOusLCpg7dJCfggJru2++YddyGOZpINzTkuH4cfH+MzIkgGC2niTXrHH/zIcf+h8Y6AczeHDQIPnbiJSdFiUq6MnI4cPM06dLpBsvtm1j7tFDhNcZ6f3nP/Ln8d570e3TRPVej9V794pdlJ1tRe5OXntNPj97trw3Pv0114h1c+KJ3t624aGHOMTTj4ZLLpGbSiQKCyW6TkuT2i/R8tZb0s6vvgpevmePCP2f/hS83FyX6dNFANPSmCdMsG76eXkifBX1bv/9b9n/ggWRt730UhHlcMfauFH2d999wcu//VbEbuhQ5s8+k3387GdybZ19PLt3yxOJn/o5s2bJ8ex21O7d8rR40UWh2+/ZI3//6enWDaAyHDokHfXDh1vZQF72UgVRQVe82b5dOrcaNw6OBs85R/5ZvUTXi717JaNnzBjvf3TjT0+bFrpNSYk8LvfrZ2UolJdbj+yDB4to+Tmvxo3F4hk1SrzMF16QTt7MzPAdfoWF8tlJk3ydMh84IE9SRFYmiF+uv16eBNw6fs84Q54ADIcOMXfsKMvMuAKTYXTxxWJBmZtpjx6SnRLOUnPjiiukY97PuIWnnpJjhSsDcOedctNxy1Z5800rTbNHD/6pY9iNsWPFiomUjnrqqcwnnRSa3WK+J+eN8+675bq1aydPpF7F7LZtkxtPJKZNs45TXs7cvbtcRzuV7ChVQVfCs3WreH516sgj/IAB/FMna0UoKgp/IzDRmPHgP/xQUi3nzmW+6ip27TfYsYP5ySfdPXMvduwQUWvVyhK61q3lUZjIu36OGThjsoj8cPiw2ExpaeLb+qG8XL5ve0eyHWM1bNokaZSmU/GLL6xtSkvFd65fX77TJ56Q/pQzzpBzrFNH0v/8cPy4PAFdfbW/7deskfb885/u648dExEeO9Z7H4cPy412wAARVK8+izlzIj8x5uTINk88Ebru0CER7W7drGMUFcnNdNw4sTkzMiTDxnkz27ZNgptatSS92IuiIkkM+NnPrGX33SdtMk/aRUUSnCxa5L2fCKigK5H54QfJZrnySsma+fnP5XE0Xhw/zvz88yJogPyzAOLfTp4c23SvY8dkQNaKFbLfgwflH7tNm9BzNI/gFRnYc+CA2C916og18t57zH/4gwiaWzaIsSOeesp9f5s2yfpGjawbkptnXVrqHrlu2yZ9IMauijQI6t57OaocfhOBtm0r/QlOTMe2X3863DUvKRGrzs02MUyeLNfOq09hwQK54Y4fL8e65x4OyuYxWVlXX231DRUWSl9Oo0Yi+JMnu+97+3YphJeebqX0MlvXcNo06S8YOFD+PioxTkIFXam+HDsmwj51qvjizgFT8WLJEumou+iiYCGZOlUiWz8dcG4UFck/thHgunUlvfTkk0OfLp5+WrYJF/X9+teSU//UU5LGF+2NrrTUSv3r0UPGJHz6aej3bCLJyZOjG4yzdKmc39lnhz6VnXtuxWw7L26/XQTT7ea4a5cI5e9+F34fxhK5/355Grn44uD1d94p6xs3lqe7U0+V/X72mYyJSEsLtZhWrZJxGA0buo8dOPVUCSD69pW/h3CjvX2ggq4objz6qPwL/OY3kpq5a5dk/fj1zr0oKBAb4ssvRcTfe8+Kru2CPHaseOJxGHwSwgcfWJ235kYzapTcVP70J1l21VUVE98XX5TP//GP1rLcXHbtDK0MmzbJ9TnllNBRzyZl1pS19qKszBq3AQRH04bly0XozZPjf/4jy7dvl+/NPkL2s8/kxtC6dfAANzsm6yUjg/m///V7tp6ooCuKG2VlzL/6lSVybdvKP3AkUagIxs548knx9j/5JL7D573Yv18skJtvls5DI2y/+EXlImkzZuKGG0TEx4yR7zI/P3ZtZ7ZsHPugq/z88Cm2TgoLpeN94sTw2y1dGtxfwSznV7u2WJTPPSdPDL16he+o37tXjuWVZhslKuiKEo4dO2Q0as+ekrMcD8rKROTsQ9qJYpv/HC3l5dKx+cEHlbdFjhwRQc3IsM5v3LjYtNPJHXfI/mfMEFukfn2xRfykWhrM9JLRkpcnIt61q7Rh5MjYjgPwQThBJ1kfHiIaCeAfANIAPM/M0xzr6wJ4FcBgAIUAJjBzXrh9Zmdnc05Ojt8qv4qS/OzfDzz5JNC4MdCnj/xkZia6VbGnpAQ4cAA44QSpGx9rSkuBkSOB+fPl/bhxMkFM586xP5Yb110HPP+8TCP56KMy+XwVQkRLmDnbdV0kQSeiNAAbAJwHIB/AdwAmMfMa2zY3AOjHzL8hookALmbmCeH2q4KuKEqF2bNHZhEbPx44/fSqPfbhw8DSpcBpp1XtcQOEE3Q/t5YhAHKZeXNgZ7MAXARgjW2biwD8JfD7WwCeIiJiP+G/oihKtDRvLk87iaB+/YSJeST8TEHXFsA22/v8wDLXbZi5FMA+ACc6d0RE1xNRDhHlFBQUVKzFiqIoiitVOqcoMz/LzNnMnJ2Zit6hoihKAvEj6NsBtLe9bxdY5roNEaUDOAHSOaooiqJUEX4E/TsAWUTUmYjqAJgIYI5jmzkAJgd+vwzAZ+qfK4qiVC0RO0WZuZSIpgD4GJK2+CIzryai+yD5kHMAvABgJhHlAiiCiL6iKIpShfhKoGTmuQDmOpbdY/v9KIBxsW2aoiiKEg1V2imqKIqixA8VdEVRlBTB19D/uByYqADADxX8eHMAe2LYnGShJp53TTxnoGaed008ZyD68+7IzK553wkT9MpARDleQ19TmZp43jXxnIGaed418ZyB2J63Wi6Koigpggq6oihKipCsgv5sohuQIGriedfEcwZq5nnXxHMGYnjeSemhK4qiKKEka4SuKIqiOFBBVxRFSRGSTtCJaCQRrSeiXCKamuj2xAMiak9EnxPRGiJaTUQ3BZY3I6JPiWhj4LVpotsaa4gojYiWEtEHgfediWhx4Hr/K1AgLqUgoiZE9BYRrSOitUR0Sg251rcE/r5XEdGbRFQv1a43Eb1IRLuJaJVtmeu1JWF64NxXENGgaI+XVIIemA5vBoBRAHoBmEREvRLbqrhQCuAPzNwLwDAAvwuc51QA85k5C8D8wPtU4yYAa23vHwLwODN3BbAXwK8S0qr48g8A85i5B4D+kPNP6WtNRG0B3Aggm5n7QAr/TUTqXe+XAYx0LPO6tqMAZAV+rgfwTLQHSypBh206PGY+DsBMh5dSMPOPzPx94PcDkH/wtpBzfSWw2SsAxiakgXGCiNoBuBDA84H3BOBsyLSGQGqe8wkARkAqloKZjzNzMVL8WgdIB5ARmEOhPoAfkWLXm5m/hFSgteN1bS8C8CoLiwA0IaLW0Rwv2QTdz3R4KQURdQIwEMBiAC2Z+cfAqp0AWiaqXXHiCQB3ACgPvD8RQHFgWkMgNa93ZwAFAF4KWE3PE1EDpPi1ZubtAP4fgK0QId8HYAlS/3oD3te20vqWbIJeoyCihgD+A+BmZt5vXxeYQCRlck6J6OcAdjPzkkS3pYpJBzAIwDPMPBDAITjslVS71gAQ8I0vgtzQ2gBogFBrIuWJ9bVNNkH3Mx1eSkBEtSFi/jozvx1YvMs8ggVedyeqfXHgNABjiCgPYqWdDfGWmwQeyYHUvN75APKZeXHg/VsQgU/law0A5wLYwswFzFwC4G3I30CqX2/A+9pWWt+STdD9TIeX9AS84xcArGXmx2yr7FP9TQbwXlW3LV4w813M3I6ZO0Gu62fMfAWAzyHTGgIpds4AwMw7AWwjou6BRecAWIMUvtYBtgIYRkT1A3/v5rxT+noH8Lq2cwBcFch2GQZgn82a8QczJ9UPgJ8B2ABgE4C7E92eOJ3jcMhj2AoAywI/P4N4yvMBbATwXwDNEt3WOJ3/mQA+CPzeBcC3AHIB/BtA3US3Lw7nOwBATuB6vwugaU241gD+CmAdgFUAZgKom2rXG8CbkD6CEsjT2K+8ri0AgmTxbQKwEpIBFNXxdOi/oihKipBslouiKIrigQq6oihKiqCCriiKkiKooCuKoqQIKuiKoigpggq6oihKiqCCriiKkiL8f3N+eOWS9C6DAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 우리 Model이 잘 만들어진 모델인지 확인!!!\n",
    "# 그래프를 이용해서 확인해보는게 좋아요!\n",
    "# 학습할때 학습데이터로 loss, acc를 구하고\n",
    "# validation data로 val_loss, val_acc를 구하게 되는데\n",
    "# 이 두 그래프를 비교해보면 overfitting의 정도를 확인!\n",
    "\n",
    "plt.plot(history.history['loss'], color='r')\n",
    "plt.plot(history.history['val_loss'], color='b')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['acc'], color='r')\n",
    "plt.plot(history.history['val_acc'], color='b')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "236/236 [==============================] - 1s 2ms/step - loss: 0.3053 - acc: 0.9046 - val_loss: 0.1703 - val_acc: 0.9469\n",
      "Epoch 2/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.1545 - acc: 0.9543 - val_loss: 0.1390 - val_acc: 0.9565\n",
      "Epoch 3/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.1128 - acc: 0.9656 - val_loss: 0.1415 - val_acc: 0.9621\n",
      "Epoch 4/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.1042 - acc: 0.9682 - val_loss: 0.1972 - val_acc: 0.9451\n",
      "Epoch 5/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0916 - acc: 0.9719 - val_loss: 0.1658 - val_acc: 0.9605\n",
      "Epoch 6/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0790 - acc: 0.9759 - val_loss: 0.1579 - val_acc: 0.9597\n",
      "Epoch 7/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0833 - acc: 0.9750 - val_loss: 0.1815 - val_acc: 0.9609\n",
      "394/394 [==============================] - 0s 600us/step - loss: 0.1546 - acc: 0.9563\n",
      "[0.1546269804239273, 0.9563491940498352]\n"
     ]
    }
   ],
   "source": [
    "# MNIST를 이용한 Deep Network으로 구현해 보아요\n",
    "# Tensorflow Keras가 제공하는 여러 기능이 있는데\n",
    "# 그 중 필수적으로 사용하는 기능들이 있어요!\n",
    "# 대표적인게 Early Stopping(조기종료) 기능이에요!\n",
    "\n",
    "# Raw Data Loading\n",
    "df = pd.read_csv(r\"C:\\Users\\lee_0\\Desktop\\코딩\\ML\\12.07\\train.csv\")\n",
    "\n",
    "# 결측치와 이상치가 없어요!\n",
    "# Feature Enginerring을 할게 없어요!\n",
    "\n",
    "# 독립변수(feature), 종속변수(target) 분리\n",
    "x_data = df.drop('label', axis=1, inplace=False).values\n",
    "t_data = df['label'].values  # 원래 one-hot 처리를 해야 해요!\n",
    "                                       # 하짐나 keras에게 one-hot처리를 위임할 수 있어서\n",
    "                                       # 따로 처리는 안할꺼예요!\n",
    "\n",
    "# 정규화는 진행해야 해요!\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(x_data)\n",
    "x_data_norm = scaler.transform(x_data)\n",
    "\n",
    "# train, test 분리\n",
    "x_data_train_norm, x_data_test_norm, t_data_train, t_data_test = \\\n",
    "train_test_split(x_data_norm,\n",
    "                 t_data,\n",
    "                 test_size=0.3,\n",
    "                 stratify=t_data,\n",
    "                 random_state=0)\n",
    "\n",
    "# Model 구현(Regression Model 구현)\n",
    "model = Sequential()\n",
    "\n",
    "# Input Layer\n",
    "model.add(Flatten(input_shape=(784,)))\n",
    "\n",
    "# Hidden Layer\n",
    "model.add(Dense(units=256,\n",
    "                activation='relu'))\n",
    "model.add(Dense(units=128,\n",
    "                activation='relu'))\n",
    "\n",
    "# Output Layer\n",
    "model.add(Dense(units=10,\n",
    "                activation='softmax'))\n",
    "model.compile(optimizer=Adam(learning_rate=1e-2),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['acc'])\n",
    "\n",
    "# Early Stopping Callback 설정\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# monitor : 조기 종료의 기준\n",
    "# patiece : loss값이 증가하는걸 몇번 참을 것 인가에 대한 숫자.\n",
    "# 1 epoch val_loss : 0.1\n",
    "# 2 epoch val_loss : 0.05\n",
    "# 3 epoch val_loss : 0.04\n",
    "# 4 epoch val_loss : 0.06 -1번 참자!!\n",
    "# 5 epoch val_loss : 0.05 -2번 참자!!\n",
    "# 6 epoch val_loss : 0.03\n",
    "es_cb = EarlyStopping(monitor='val_loss',\n",
    "                      patience=5,\n",
    "                      restore_best_weights=True)\n",
    "\n",
    "# model.summary()\n",
    "\n",
    "# Model 학습\n",
    "history = model.fit(x_data_train_norm,\n",
    "                    t_data_train,\n",
    "                    epochs=100,\n",
    "                    batch_size=100,\n",
    "                    validation_split=0.2,\n",
    "                    callbacks=[es_cb],\n",
    "                    verbose=1)\n",
    "\n",
    "# Model 평가\n",
    "print(model.evaluate(x_data_test_norm,\n",
    "                     t_data_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
